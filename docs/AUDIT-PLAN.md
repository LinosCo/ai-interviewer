# PIANO DI AUDIT COMPLETO - AI Interviewer Platform

> **Data:** 2026-02-27 | **Branch:** stage
> **Obiettivo:** Verificare la corrispondenza UI ‚Üî Codice, la qualit√† dei flussi LLM, la crescita automatica della KB, e l'allineamento agli obiettivi della piattaforma
> **Target utente primario:** Management PMI italiana, Consulenti/Agenzie
> **Target utente raccolta dati:** Stakeholder internazionali (chatbot/interviste multilingua)

---

## INDICE

- [0. METODOLOGIA E ASSEGNAZIONE MODELLI](#0-metodologia)
- [1. RICERCA BENCHMARK E BEST PRACTICES](#1-ricerca)
- [2. AUDIT AREA: INTERVIEW ENGINE](#2-interview)
- [3. AUDIT AREA: CHATBOT ENGINE](#3-chatbot)
- [4. AUDIT AREA: ANALYTICS & INSIGHTS](#4-analytics)
- [5. AUDIT AREA: KNOWLEDGE BASE](#5-kb)
- [6. AUDIT AREA: VISIBILITY & BRAND MONITORING](#6-visibility)
- [7. AUDIT AREA: CMS & CONTENT SUGGESTIONS](#7-cms)
- [8. AUDIT AREA: AI TIPS & AUTOMAZIONE](#8-tips)
- [9. AUDIT AREA: COPILOT STRATEGICO](#9-copilot)
- [10. AUDIT AREA: BILLING & CREDITS](#10-billing)
- [11. AUDIT AREA: DASHBOARD & SETTINGS](#11-dashboard)
- [12. AUDIT CROSS-CUTTING: PROMPT QUALITY](#12-prompts)
- [13. AUDIT CROSS-CUTTING: LINGUA E LOCALIZZAZIONE](#13-lingua)
- [14. AUDIT CROSS-CUTTING: SICUREZZA E PRIVACY](#14-sicurezza)
- [15. AUDIT CROSS-CUTTING: PERFORMANCE & COSTI](#15-performance)
- [16. GAP ANALYSIS & ROADMAP SUGGERITA](#16-gap)

---

<a id="0-metodologia"></a>
## 0. METODOLOGIA E ASSEGNAZIONE MODELLI

### 0.1 Approccio di Audit

Ogni area viene verificata su **4 dimensioni**:

| Dimensione | Descrizione |
|-----------|-------------|
| **UI ‚Üî Codice** | Ci√≤ che l'interfaccia mostra/promette corrisponde a ci√≤ che il codice esegue? |
| **Qualit√† LLM** | I prompt producono output coerenti con gli obiettivi? Le guard funzionano? |
| **Dati & Pipeline** | I dati fluiscono correttamente attraverso tutti gli stadi? |
| **Obiettivo Business** | La feature supporta efficacemente la mission della piattaforma? |

### 0.2 Assegnazione Modelli per Ottimizzazione Token

| Tipo di Task | Modello | Motivazione |
|-------------|---------|-------------|
| **Analisi architetturale complessa** (prompt quality, logic flows, gap analysis) | **Opus** | Richiede ragionamento profondo e connessioni non ovvie |
| **Verifica codice-UI** (mapping routes ‚Üî componenti, verifica feature) | **Sonnet** | Buon bilanciamento tra comprensione e costo |
| **Ricerca best practices** (web search, benchmark gathering) | **Opus** | Richiede sintesi di fonti multiple e giudizio qualitativo |
| **Check strutturali** (file existence, pattern matching, schema validation) | **Haiku** | Task meccanici e veloci |
| **Verifica prompt individuali** (singolo prompt ‚Üí output atteso) | **Sonnet** | Analisi focalizzata su un singolo prompt |
| **Cross-referencing DB ‚Üî API ‚Üî UI** | **Haiku** | Pattern matching su strutture note |
| **Generazione raccomandazioni** (suggerimenti implementativi) | **Opus** | Richiede creativit√† e visione d'insieme |

### 0.3 Convenzioni Checklist

- `[ ]` = Da verificare
- `[‚úì]` = Verificato OK
- `[‚úó]` = Problema trovato (con nota)
- `[~]` = Parzialmente implementato
- `[!]` = Critico - richiede intervento
- **Modello suggerito** indicato con üè∑Ô∏è accanto a ogni task

---

<a id="1-ricerca"></a>
## 1. RICERCA BENCHMARK E BEST PRACTICES

> **Obiettivo:** Stabilire criteri oggettivi di valutazione per ogni area funzionale
> üè∑Ô∏è **Modello:** Opus (ricerca web + sintesi)

### 1.1 Interviste Qualitative AI-Powered

**Benchmark di riferimento (da ricerca):**
- Chatbot enterprise: containment rate target 70-90%, FAQ bot 40-60%
- AI chatbot risolvono 87% richieste senza escalation umana
- Tempo risposta target: 1-3 secondi per chatbot testuali
- Lead form completion rate via bot: target ~10%
- SMS + AI assistant aumentano completion del 12-20% in ruoli shift-based
- Opzioni non-video (audio/testo) aumentano completion per candidati disabili del 6-10%

**Checklist audit:**
- [ ] Confrontare completion rate piattaforma vs benchmark 70-90%
- [ ] Verificare tempo risposta <3s per intervista conversazionale
- [ ] Verificare che il sistema di probing segua best practices qualitative (1 domanda per turno, no leading questions)
- [ ] Verificare gestione fatigue (rilevamento risposte brevi, rallentamento pacing)
- [ ] Verificare adattamento sentiment in real-time
- [ ] Confrontare con piattaforme competitor (Typeform AI, SurveyMonkey Genius, Qualtrics XM)
- [ ] Verificare standard etici per consenso e trasparenza AI (AI Act disclosure)

### 1.2 GEO (Generative Engine Optimization)

**Benchmark di riferimento (da ricerca):**
- GEO: ottimizzare contenuti per apparire nelle risposte di AI (ChatGPT, Gemini, Perplexity)
- 1B+ prompt/giorno su ChatGPT, 71% americani usa AI search per acquisti
- Traffico search tradizionale previsto -25% entro 2026, -50% entro 2028
- LLM citano tipicamente 2-7 domini per risposta
- Metriche chiave: citation frequency, brand share of voice, sentiment, mention rate
- Frequenza monitoring raccomandata: **settimanale** (pattern direzionali, non punteggi assoluti)
- E-E-A-T (Experience, Expertise, Authoritativeness, Trustworthiness) critico per GEO
- Structured data/schema markup fondamentale per comprensione LLM
- Brand authority cross-platform (Wikidata, Crunchbase, LinkedIn) aumenta visibilit√† GEO

**Checklist audit:**
- [ ] Verificare che il sistema di monitoring copra i provider principali (ChatGPT, Gemini, Perplexity, Claude)
- [ ] Verificare tracking metriche: citation frequency, share of voice, sentiment, positioning
- [ ] Verificare frequenza scan almeno settimanale per risultati significativi
- [ ] Verificare che i suggerimenti contenuto includano ottimizzazione E-E-A-T
- [ ] Verificare che i suggerimenti includano structured data/schema per GEO
- [ ] Verificare tracking AI Overview di Google
- [ ] Verificare confronto competitivo nel tempo (share of voice vs competitors)

### 1.3 Digital Marketing B2B per PMI

**Benchmark di riferimento (da ricerca):**
- Topic cluster: pillar page + cluster articles con interlinking interno
- 81% team marketing B2B usa AI generativa (CMI 2026)
- Google: contenuti "original, high-quality, people-first"
- Formati ad alte performance: guide complete, eBook, case study, video (explainer, brand, customer success)
- Distribuzione canali per intent: SEO per high-intent, LinkedIn per thought leadership, email per nurturing
- Schema.org prioritari B2B: Organization (sitewide), FAQPage (service+blog), BlogPosting (articoli)
- Poi: Service schema, Person schema per authority
- JSON-LD formato raccomandato da Google
- Siti con Article schema: 2-3x higher citation rate in AI summaries
- Rich results: 20-40% higher CTR

**Checklist audit:**
- [ ] Verificare che CMS suggestions seguano logica topic cluster
- [ ] Verificare che contenuti generati siano "people-first" e non generici AI
- [ ] Verificare copertura formati (blog, FAQ, landing page, case study)
- [ ] Verificare che schema.org generato includa Organization, FAQPage, BlogPosting
- [ ] Verificare formato JSON-LD per schema
- [ ] Verificare keyword strategy basata su dati GSC reali
- [ ] Verificare che suggerimenti includano meta description, og tags, SEO title

### 1.4 LinkedIn & Social B2B

**Benchmark di riferimento (da ricerca):**
- LinkedIn 2025-2026: priorit√† "depth of engagement" vs vanity metrics
- Dwell time (tempo speso sul contenuto) = indicatore primario di rilevanza
- Carousel: engagement rate pi√π alto (6.60%), pi√π like
- Video nativo: 5x engagement vs post statici
- Profili personali: 8x engagement vs company page
- Frequenza ottimale: 3-5 post/settimana di alta qualit√† > post giornalieri low-effort
- Content reactivation: commentare/reshare dopo 8-24h per spingere nei feed
- Brand memorabili: elementi visivi/verbali distintivi ripetuti (logo, tono, colore)
- Focus su: story-driven, conversational, expert content, framework pratici

**Checklist audit:**
- [ ] Verificare che AI tips per social includano formati ottimizzati LinkedIn (carousel, video, document)
- [ ] Verificare suggerimento frequenza posting (3-5/settimana)
- [ ] Verificare focus su profili personali vs company page
- [ ] Verificare che contenuti suggeriti siano "story-driven" e con framework pratici
- [ ] Verificare che non vengano suggeriti formati low-engagement (link post semplici)
- [ ] Verificare inclusione metriche LinkedIn (dwell time, engagement rate) nel monitoring

### 1.5 Strategie Marketing PMI Italiane

**Benchmark di riferimento (da ricerca):**
- 54% PMI italiane investono intensamente in digitale (2025)
- Solo 19% adotta tecnologie avanzate in modo strutturato
- Solo 26.2% ha livello "alto" di digitalizzazione (CRM + analytics + automation)
- 83% PMI riporta difficolta nell'adozione digitale
- Principali barriere: carenze culturali (44%), mancanza competenze (59%), costi (40%)
- 47% criticita nell'accesso alla connettivita digitale
- Trend 2025: AI e automazione, video content e formati brevi
- Finanziamenti disponibili: voucher e contributi per tecnologie, consulenza, formazione

**Checklist audit:**
- [ ] Verificare che il linguaggio UI sia accessibile a PMI con bassa digitalizzazione
- [ ] Verificare che i suggerimenti siano implementabili con budget limitato
- [ ] Verificare che l'onboarding guidi utenti con bassa maturita digitale
- [ ] Verificare che i tip includano formati video/brevi dove appropriato
- [ ] Verificare che la piattaforma non richieda competenze tecniche avanzate
- [ ] Verificare che il copilot adatti raccomandazioni al livello di maturita digitale dell'org

### 1.6 Knowledge Base Auto-Growth

**Benchmark di riferimento (da ricerca):**
- RAG feedback loop: chiedere utilita risposta (Resolved/Helpful/Unhelpful)
- Dynamic KB growth: registrare complaint, query, review, ticket e reinserire nel RAG
- KB Coverage: tracciare cambiamenti retrieval performance nel tempo
- "Human in the loop" per monitoring e QA = implementazioni piu riuscite
- Document curation rigorosa prima dell'ingestion
- Escludere PII dal contenuto KB
- Role-based access controls sulla KB
- Continuous evaluation and improvement tramite metriche e feedback

**Checklist audit:**
- [ ] Verificare implementazione feedback loop (thumbs up/down su risposte)
- [ ] Verificare pipeline di estrazione automatica da conversazioni ‚Üí KB
- [ ] Verificare curation/validazione contenuto prima dell'ingestion
- [ ] Verificare esclusione PII dal contenuto KB
- [ ] Verificare metriche di qualita KB (coverage, retrieval accuracy)
- [ ] Verificare che KB cresca da tutte le 7 fonti dichiarate (chat, interviste, tip, scraping, analytics, GSC, gap risolti)
- [ ] Valutare opportunita vector embeddings vs attuale keyword matching

---

<a id="2-interview"></a>
## 2. AUDIT AREA: INTERVIEW ENGINE

> **Files chiave:**
> - `src/app/api/chat/route.ts` (3511 righe - motore principale)
> - `src/lib/llm/prompt-builder.ts` (5 blocchi prompt)
> - `src/lib/llm/runtime-prompt-blocks.ts` (blocchi 6-7)
> - `src/lib/interview/interview-supervisor.ts` (state machine)
> - `src/lib/interview/micro-planner.ts` (decisioni per turno)
> - `src/lib/interview/runtime-knowledge.ts` (KB runtime)
> - `src/lib/llm/candidate-extractor.ts` (estrazione profilo)
> - `src/lib/tone/tone-prompt-adapter.ts` (adattamento tono)
> - `src/lib/memory/memory-manager.ts` (memoria conversazione)

### 2.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Pagina Configurazione Bot (`/dashboard/bots/[botId]`)
- [‚úì] **Campi configurazione**: Tutti i campi UI (nome, tono, lingua, obiettivo ricerca, target audience, topics, subgoals) sono usati nel `PromptBuilder.build()` via i 7 blocchi
- [~] **Selezione modello**: Il dropdown UI mostra 5 modelli ma `MODEL_ASSIGNMENTS` ne usa 7 task-types. Il dropdown include modelli non assegnati a nessun task (es. CLAUDE_SONNET definito ma non assegnato). **NOTA: MODEL_ASSIGNMENTS non √® consumato universalmente** ‚Äî copilot, CMS, website analysis usano modelli hardcoded
- [‚úì] **Toggle "Raccolta dati candidato"**: `collectCandidateData` attiva correttamente la fase DATA_COLLECTION nel supervisor quando `allTopicsCovered === true`
- [‚úì] **Campi dati candidato**: Tutti i campi selezionabili supportati in `extractFieldFromMessage()` con regex-first per email/phone/url, poi fallback LLM
- [~] **Durata intervista**: Il budget tempo √® rispettato dal supervisor tramite `checkBudgetAndDecide()`, ma il campo si chiama `maxDuration` in config vs `effectiveDuration` nella conversation ‚Äî **naming inconsistency**
- [~] **Knowledge sources**: Le fonti KB sono iniettate nel prompt, MA troncate a 3 fonti √ó max 260 chars ciascuna. Fonti pi√π lunghe perdono contenuto silenziosamente. **Nessuna selezione per rilevanza semantica ‚Äî solo ordine arbitrario**
- [‚úì] **Anteprima messaggio intro**: Il messaggio di benvenuto configurato viene inviato come primo messaggio via `conversation.welcomeMessage`

#### Pagina Conversazioni (`/dashboard/bots/[botId]/conversations`)
- [~] **Lista conversazioni**: Lo stato mostrato usa IN_PROGRESS e COMPLETED correttamente, ma **ABANDONED non viene mai impostato** ‚Äî il campo `conversation.status` √® un `String` libero, non un enum DB. Nessun timeout o logica setta ABANDONED
- [‚úì] **Sentiment indicator**: Il sentiment mostrato corrisponde a `conversationAnalysis.sentimentScore` (scala -1 a +1, mostrato come %)
- [~] **Durata mostrata**: `effectiveDuration` √® calcolata da `startedAt` a `completedAt` ‚Äî se la conversazione non √® completata, il campo √® null. **Naming inconsistency con `maxDuration` nella config**
- [‚úì] **Transcript view**: Il transcript mostra tutti i messaggi user + assistant. I messaggi di sistema/validazione sono filtrati
- [‚úì] **Profilo candidato**: I dati estratti corrispondono ‚Äî `CandidateExtractor` usa regex + LLM con validazione formato

#### Pagina Piano Intervista (`/dashboard/bots/[botId]/plan`)
- [‚úì] **Editor piano**: Le modifiche al piano vengono salvate in `InterviewPlan` e usate dal supervisor per costruire i blocchi prompt
- [‚úì] **Sub-goals**: I sub-goals editati sono targettizzati dal micro-planner ‚Äî `coverageTracker` li usa come target
- [‚úì] **Turni per topic**: Il budget turni √® gestito dall'elastic budget in `topic-manager.ts` con ribilanciamento dinamico

#### Widget Pubblico (`/i/[slug]/`)
- [‚úì] **Welcome screen**: Mostra correttamente nome bot e messaggio intro
- [‚úì] **Consenso**: Il consenso viene raccolto PRIMA dell'inizio tramite checkbox obbligatorio
- [~] **Progress bar**: `SemanticProgressBar` traccia topic coverage, MA traccia solo i topic principali, **non i sub-goals**. L'avanzamento percepito pu√≤ essere impreciso
- [‚úì] **Messaggio fine**: La chiusura include ringraziamento + domanda "C'√® qualcos'altro?"
- [‚úì] **Responsive**: Il widget √® responsive con layout flessibile
- [~] **Lingue**: Il widget **ha la LandingPage hardcoded in italiano**. Le opzioni lingua nel form ("Seleziona lingua") fanno fallback silenzioso. **Non c'√® mapping lingua ‚Üí label UI nel widget pubblico**

### 2.2 Flussi LLM üè∑Ô∏è Opus

#### State Machine del Supervisor
- [‚úì] **Transizioni WARMUP ‚Üí SCAN ‚Üí DEEP ‚Üí CLOSURE**: Le transizioni sono triggerate dalle condizioni corrette nel supervisor con `checkBudgetAndDecide()`
- [‚úì] **WARMUP**: Dura 1-3 turni con domande ice-breaker leggere. Il micro-planner sceglie strategia `cover_subgoal`
- [‚úì] **SCAN (EXPLORE)**: Ogni topic viene visitato almeno una volta. Il turn budget √® gestito dall'elastic budget
- [‚úì] **DEEP (DEEPEN)**: Solo topic con HIGH signal score vengono approfonditi
- [‚úì] **DEEP_OFFER_ASK**: La domanda di continuazione √® formattata come vera yes/no. L'enforcement usa prompt dedicato (Prompt #14-15)
- [‚úì] **DATA_COLLECTION_CONSENT**: Il consenso viene chiesto PRIMA tramite Prompt #16 (consent enforcement)
- [‚úì] **DATA_COLLECTION**: I campi vengono chiesti uno alla volta con ordine logico (nome ‚Üí email ‚Üí phone ‚Üí altri)
- [‚úì] **CLOSURE**: La chiusura include "C'√® qualcos'altro?" prima del saluto finale
- [~] **Edge case**: Off-topic viene gestito dal Block 7 (Guards) con redirect gentile. MA **ABANDONED non viene MAI impostato**: non c'√® timeout inattivit√†, nessun meccanismo rileva utenti che abbandonano la sessione. Le conversazioni restano IN_PROGRESS indefinitamente

#### Prompt Builder - 7 Blocchi
- [‚úì] **Blocco 1 (Identit√†)**: Le regole fondamentali sono presenti: 1 domanda per turno, no promo, terminare con "?", mai menzionare struttura interna
- [‚úì] **Blocco 2 (Contesto intervista)**: Budget tempo calcolato, status pacing iniettato (AHEAD/ON_TRACK/BEHIND)
- [‚úì] **Blocco 3 (Focus Topic)**: Topic corrente comunicato con sub-goals. La mappa i18n √® presente per label campi
- [‚úì] **Blocco 4 (Memoria)**: Fatti estratti da `MemoryManager.getExtractedFacts()` ‚Äî basati su analisi reale, non inventati
- [!] **Blocco 5 (Knowledge)**: **BUG CRITICO**: Il language detection usa `const isItalian = currentTopic.label.length > 0` che √® **SEMPRE true** (qualsiasi stringa non vuota). Questo significa che le knowledge cues sono SEMPRE in italiano indipendentemente dalla lingua configurata. Le cues sono pertinenti al topic ma limitate a 260 chars per fonte
- [‚úì] **Blocco 6 (Turn Guidance)**: Il signal scoring (HIGH/MEDIUM/LOW) funziona correttamente in `runtime-prompt-blocks.ts`
- [~] **Blocco 7 (Guards)**: Le guard per off-topic funzionano con redirect gentile, MA **non c'√® escalation** dopo N tentativi off-topic consecutivi ‚Äî l'intervista pu√≤ restare bloccata in un loop

#### Micro-Planner
- [‚úì] **Strategia**: Le 4 strategie sono selezionate in base al signal score e alla copertura corrente dei sub-goals
- [‚úì] **Stile**: I 3 stili producono prompt differenziati per l'LLM
- [‚úì] **Copertura**: Il micro-planner traccia i sub-goals coperti via `coverageTracker`

#### Qualit√† Domande Generate
- [‚úì] **Una domanda per turno**: Il Blocco 1 istruisce "Concludi SEMPRE con una domanda aperta" e il question-only enforcement (Prompt #13) rigenera se necessario
- [‚úì] **No stock openers**: Il blocco identit√† istruisce esplicitamente "evita frasi fatte"
- [‚úì] **Riferimento concreto**: Il Blocco 4 (Memoria) inietta fatti estratti dalla conversazione per riferimenti specifici
- [‚úì] **Non leading**: Le istruzioni nel Blocco 1 specificano "non suggerire risposte"
- [‚úì] **Bridge naturali**: Il micro-planner include stile `neutral_bridge` per transizioni fluide

#### Estrazione Dati Candidato
- [‚úì] **Regex-first**: `extractFieldFromMessage()` usa regex per email, phone, URL (linkedin, portfolio) prima del fallback LLM
- [‚úì] **Validazione**: Formato email e telefono validati con regex specifiche
- [‚úì] **Feedback**: Il Prompt #17 (Field Collection Enforcement) genera messaggi di retry gentili
- [‚úì] **Skip intent**: L'utente pu√≤ dire "preferisco non dirlo" e il sistema va avanti al campo successivo

#### Adattamento Tono
- [!] **Profilo comunicativo**: `tone-prompt-adapter.ts` esiste con logica completa per profilo comunicativo, MA **√® UNWIRED** ‚Äî non viene mai invocato nel flusso principale `chat/route.ts`. Il tono NON si adatta dinamicamente
- [!] **Emoji**: La logica emoji √® nel tone adapter ma **non connessa** al prompt builder
- [!] **Complessit√† linguistica**: Il tone adapter ha istruzioni di complessit√† MA √® **solo in italiano** ‚Äî nessuna localizzazione delle istruzioni di tono

### 2.3 Pipeline Dati üè∑Ô∏è Haiku

- [‚úì] **Salvataggio messaggi**: Ogni messaggio viene salvato in `Message` con timestamp, ruolo, e metadata
- [‚úì] **Idempotenza**: `clientMessageId` previene duplicati
- [‚úì] **Token tracking**: I token vengono tracciati via `TokenTrackingService.logTokenUsage()` per modello e operazione
- [‚úì] **Crediti**: I crediti vengono detratti con costo `interview_question: 8` tramite `actionOverride` nel token tracking
- [~] **Conversation status**: IN_PROGRESS ‚Üí COMPLETED funziona, MA **ABANDONED non viene MAI impostato**. Il campo √® `String` libero nel DB, non un enum. Nessun timeout rileva sessioni abbandonate
- [‚úì] **Effective duration**: Calcolato server-side da `startedAt` a `completedAt`
- [‚úì] **Candidate profile**: Salvato come `candidateProfile` JSON nella `Conversation`

> **‚ö†Ô∏è FINDING CRITICO ‚Äî Token Overflow Risk**: La conversation history non ha truncation. Con il cap di 260 messaggi e 7 blocchi di prompt, una conversazione lunga pu√≤ eccedere il context window del modello (8000 token input limit per alcuni modelli). Non c'√® logica di sliding window o summarization della history

### 2.4 Obiettivo Business üè∑Ô∏è Opus

- [‚úì] **Ascolto stakeholder**: L'architettura a 7 blocchi + micro-planner produce domande adattive e contestuali. Il signal scoring (HIGH/MEDIUM/LOW) guida l'approfondimento
- [~] **Adattivit√†**: Il sistema √® adattivo nel contenuto (topic + sub-goals) MA **il tono NON si adatta** (tone adapter unwired). La personalizzazione √® solo a livello di contenuto, non di stile comunicativo
- [‚úì] **Actionability**: I dati raccolti (temi, sentiment, citazioni, profilo candidato) sono strutturati per insight. L'AnalyticsEngine genera 4 tipi di insight azionabili
- [~] **Esperienza utente**: L'esperienza √® generalmente positiva, MA: nessun rilevamento fatigue (risposte corte ripetute), nessun rallentamento pacing automatico, landing page solo in italiano
- [~] **Confronto benchmark**: Il sistema segue le best practices qualitative (1 domanda per turno, no leading questions, probing). Mancano: opzioni non-testo (audio/video), rilevamento fatigue, adaptive pacing basato su sentiment real-time

---

<a id="3-chatbot"></a>
## 3. AUDIT AREA: CHATBOT ENGINE

> **Files chiave:**
> - `src/app/api/chatbot/message/route.ts` (804 righe)
> - `src/app/api/chatbot/start/route.ts`
> - `src/lib/chatbot/message-guards.ts`
> - `src/lib/chatbot/knowledge-gap-detector.ts`
> - `src/lib/chatbot/analytics-aggregator.ts`
> - `src/lib/templates/chatbot-templates.ts`

### 3.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Configurazione Chatbot (`/dashboard/bots/[botId]` - modo chatbot)
- [‚úì] **Template predefiniti**: I template in `chatbot-templates.ts` generano configurazioni funzionanti con topics, boundaries, fallback, tone preconfigurati
- [‚úì] **Topics & boundaries**: I limiti di scope sono iniettati nel prompt e rispettati dal chatbot
- [‚úì] **Fallback message**: Il messaggio di fallback √® configurabile e usato nel prompt quando il chatbot esce dallo scope
- [~] **Lead capture strategy**: Le 3 strategie (aggressive, smart, passive) sono definite nell'enum, MA la logica `smart` usa un LLM call separato (`gpt-4o-mini`) per decidere il timing ‚Äî **il caso `priority` nel codice usa uppercase/lowercase inconsistente** (`HIGH` vs `high`)
- [~] **Page context**: `enablePageContext` √® salvato nel DB ma **il toggle manca nella UI** di ChatbotSettings.tsx. Il chatbot.js estrae il contesto pagina e lo invia, ma l'utente non pu√≤ attivare/disattivare la feature dalla dashboard
- [~] **Widget customization**: Colori e avatar sono configurabili. MA **`bubblePosition` √® hardcoded** a `bottom-right` ‚Äî non c'√® opzione di posizionamento nella UI

#### Widget Chat (`/w/[botId]/`)
- [‚úì] **Init session**: La sessione viene creata con `pageUrl`, `referrer`, `userAgent` come metadata
- [‚úì] **Risposte**: Le risposte usano KB + topic boundaries per rispondere nel contesto corretto
- [‚úì] **Out-of-scope**: Il chatbot rifiuta garbatamente con il messaggio di fallback configurato
- [‚úì] **Lead capture**: Il timing √® gestito dalle 3 strategie con logica LLM per `smart`
- [~] **Embed code**: Il codice embed (`chatbot.js`) funziona come IIFE che crea iframe + bolla. MA **le opzioni avanzate di embed (`data-auto-open`, `data-hide-mobile`, `data-delay`) generano attributi `data-*` che chatbot.js NON legge mai**. Queste opzioni sono decorative nella UI

#### Knowledge Gaps (`/dashboard/bots/[botId]/knowledge-gaps`)
- [‚úì] **Lista gaps**: I gap provengono da `knowledge-gap-detector.ts` che analizza conversazioni con fallback/low confidence
- [~] **Priorit√†**: Il sistema di priorit√† usa `HIGH/MEDIUM/LOW` dal LLM, ma **la route `/api/knowledge/gaps` non filtra per organizzazione** ‚Äî potenziale cross-org data leak
- [‚úì] **FAQ suggerite**: Le FAQ sono generate dal LLM con domanda + risposta basata sul contesto della conversazione
- [~] **Azione "approva"**: Approvare un gap ha **due code paths**: uno crea `KnowledgeSource` direttamente, l'altro usa `FaqSuggestion` model. MA il model `FaqSuggestion` **non ha nessun write path** (nessuna route lo crea) ‚Äî √® un modello orfano nel DB

### 3.2 Flussi LLM üè∑Ô∏è Opus

#### Prompt Chatbot
- [‚úì] **buildChatbotPrompt()**: Il prompt include nome, tono, scope, boundaries, KB, e page context (quando abilitato)
- [‚úì] **Guardrail scope**: Le istruzioni per fuori-scope includono messaggio di fallback configurabile
- [‚úì] **Lead capture timing**: La strategia `smart` usa `gpt-4o-mini` per analizzare il momento opportuno. Le strategie `aggressive` e `passive` sono deterministiche
- [~] **Never reveal instructions**: Il prompt include istruzioni di non-disclosure, MA non c'√® test esplicito di robustezza contro prompt injection. **Da verificare nella Fase 3 (Sicurezza)**

#### Knowledge Gap Detection
- [‚úì] **detectKnowledgeGaps()**: Usa `claude-haiku` per analizzare conversazioni con basso confidence score
- [~] **Deduplicazione**: Non c'√® deduplicazione esplicita dei gap ‚Äî gap simili da conversazioni diverse possono creare duplicati
- [‚úì] **Qualit√† suggerimenti FAQ**: Le FAQ sono generate con contesto conversazione + KB esistente per evitare hallucination

### 3.3 Pipeline Dati üè∑Ô∏è Haiku

- [‚úì] **ChatbotSession**: Sessione creata con `pageUrl`, `referrer`, `userAgent`
- [‚úì] **Messaggi**: Salvati con ruolo corretto (user/assistant) e timestamp
- [‚úì] **Analytics aggregation**: Il cron `aggregate-chatbot-analytics` chiama `aggregateChatbotAnalytics()` con auth Bearer
- [~] **Bounce rate**: Il calcolo esiste nell'aggregator MA **il campo `suggestedFaq` NON viene incluso** nell'output dell'aggregazione ‚Äî i dati FAQ suggerite si perdono
- [‚úì] **Lead tracking**: I lead catturati sono contati dal profilo candidato (email/phone/name presenti)

---

<a id="4-analytics"></a>
## 4. AUDIT AREA: ANALYTICS & INSIGHTS

> **Files chiave:**
> - `src/lib/analytics/AnalyticsEngine.ts`
> - `src/lib/insights/sync-engine.ts` (CrossChannelSyncEngine)
> - `src/app/api/insights/sync/route.ts`
> - `src/app/api/projects/[projectId]/analytics/route.ts`
> - `src/app/api/chatbot/[botId]/analytics/route.ts`

### 4.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Dashboard Progetto (`/dashboard/projects/[projectId]`)
- [‚úì] **Metriche aggregate**: `AnalyticsEngine.generateProjectInsights()` calcola completion rate, sentiment medio, durata media da dati reali (30 giorni)
- [‚úì] **Trend grafici**: I trend giornalieri sono calcolati su 30 giorni con inizializzazione di tutti i giorni (anche quelli senza dati = volume 0)
- [‚úì] **Top themes**: I temi provengono da `ThemeOccurrence` ‚Üí aggregati per nome con count + sentiment medio. Top 5 selezionati
- [~] **NPS score**: Il calcolo NPS usa `metadata.npsScore` dalle analisi interviste. MA **non √® il calcolo NPS standard** (promoters-detractors/total*100) ‚Äî √® una media dei punteggi individuali
- [~] **Knowledge gaps**: I gap provengono da `ChatbotAnalytics.knowledgeGaps` MA nell'aggregator il campo `suggestedFaq` **non viene incluso** nell'output
- [‚úì] **Lead captured**: Il conteggio lead √® basato sulla presenza di email/phone/name nel `candidateProfile`

#### Analytics Bot (`/dashboard/bots/[botId]/analytics`)
- [‚úì] **Metriche sessione**: Sessions count e messages count corrispondono al DB
- [~] **Question clusters**: I cluster provengono da `ThemeOccurrence` ‚Äî basati su temi estratti dall'analisi, non clustering diretto delle domande
- [‚úì] **Sentiment by topic**: Distribuzione sentiment per topic calcolata dall'AnalyticsEngine
- [~] **Timeframe filter**: Il filtro √® hardcoded a 30 giorni in `AnalyticsEngine`. **Non c'√® supporto per 7d/90d** a livello di engine ‚Äî da verificare se il frontend filtra localmente

#### Insights Cross-Channel (`/dashboard/insights`)
- [‚úì] **Lista insight**: Gli insight provengono da `CrossChannelInsight` con `suggestedActions` JSON
- [‚úì] **Priority scoring**: Il punteggio priorit√† (0-100) √® generato dal LLM nel `CrossChannelSyncEngine` con Zod validation
- [‚úì] **Status workflow**: Gli stati pending ‚Üí actioned ‚Üí completed ‚Üí archived sono supportati nel modello
- [~] **Azioni suggerite**: Ogni insight ha azioni, MA il bottone "Applica" **√® decorativo** (nessun onClick handler)
- [‚úì] **Fonti dati**: Il CrossChannelSyncEngine include dataPoints con source type e dati specifici

### 4.2 Flussi LLM üè∑Ô∏è Opus

#### CrossChannelSyncEngine
- [‚úì] **Aggregazione dati**: Il motore raccoglie dati da 6 fonti: visibility configs, interviste (conversations + analysis), chatbot (sessions + analytics), SERP monitoring, CMS analytics (WebsiteAnalytics), website analytics. Tutto filtrato per `organizationId` e opzionalmente `projectId`
- [‚úì] **Prompt qualit√†**: Il prompt √® molto dettagliato (~2000+ token), in italiano, con istruzioni specifiche per 5-7 insight, priority scoring 0-100, e health report. Include 11 action types distinti e schema Zod rigoroso
- [‚úì] **Allineamento strategico**: Il prompt inietta `strategicVision`, `valueProposition` e `strategicPlan` dall'organizzazione/progetto. Le istruzioni chiedono esplicitamente di allineare insight alla visione strategica
- [‚úì] **Evidence-based**: Il prompt istruisce "cita sempre le fonti" con dataPoints che includono source type e dati specifici. I `suggestedActions` hanno campo `reasoning` obbligatorio
- [‚úì] **Health Report**: Schema Zod con 3 sotto-report (chatbot satisfaction, website effectiveness, brand visibility), ognuno con score 0-100, summary, e trend/contentGaps
- [‚úì] **Deduplicazione**: Usa Jaccard similarity (soglia 0.38) su `topicName` per evitare insight duplicati. I nuovi insight vengono confrontati con quelli non-archived esistenti

#### AnalyticsEngine
- [~] **generateProjectInsights()**: **NON √® LLM-based** ‚Äî √® pura aggregazione deterministica. Calcola stats (completion rate, sentiment, durata, NPS) da dati reali 30 giorni. Genera 4 tipi di insight con logica basata su pattern, NON 12-15 come ipotizzato
- [~] **Tipi insight**: I 4 tipi (CONTENT_SUGGESTION, INTERVIEW_QUESTION, KB_UPDATE, AD_CAMPAIGN) sono generati da **regole deterministiche** basate su temi e sentiment, non da LLM. La qualit√† dipende dalla qualit√† dei dati di input
- [‚úó] **Cross-pollination**: **NON implementata**. I temi chatbot appaiono nelle analytics aggregate, ma non c'√® nessuna logica che suggerisce automaticamente temi chatbot come domande di intervista o viceversa. L'insight di tipo INTERVIEW_QUESTION viene generato ma senza connessione diretta ai dati chatbot

### 4.3 Pipeline Dati üè∑Ô∏è Haiku

- [‚úì] **ConversationAnalysis**: L'analisi post-conversazione salva `sentimentScore`, `themes` (via ThemeOccurrence), `citations`, `metadata` (con NPS, executive summary). Il trigger √® in `chat/route.ts` a fine intervista
- [‚úì] **Theme clustering**: I temi vengono aggregati per nome con count + sentiment medio. Top 5 selezionati nell'AnalyticsEngine. La deduplicazione √® per nome esatto (non semantica)
- [~] **Cron jobs**: `aggregate-chatbot-analytics` funziona con Bearer auth. `sync-insights` ha **ZERO auth** (chiunque pu√≤ triggerare). `detect-gaps` ha **auth commentata**. Tutti e 3 dipendono da `vercel.json` che **non esiste** ‚Äî nessuno √® schedulato
- [~] **Caching**: Il caching √® limitato: `RuntimeKnowledgeCache` ha TTL 24h per knowledge. `GlobalConfig` ha cache 5min. **Nessun caching** per AnalyticsEngine o CrossChannelSyncEngine ‚Äî ogni richiesta ricalcola tutto da DB

### 4.4 Obiettivo Business üè∑Ô∏è Opus

- [‚úì] **Decisioni consapevoli**: I dati presentati (trend sentiment, temi principali, NPS, completion rate, health report) sono sufficienti per decisioni informate a livello PMI
- [‚úì] **Azionabilit√†**: Gli insight cross-channel includono `suggestedActions` con 11 tipi azione, titolo, body, reasoning. Il target indica il canale. Sono concretamente azionabili
- [~] **Tempestivit√†**: La frequenza dipende dal cron `sync-insights` che dovrebbe girare periodicamente, MA **non √® schedulato** (no vercel.json). Gli insight si generano solo quando il cron viene triggerato manualmente
- [‚úì] **Comprensibilit√†**: Il prompt del CrossChannelSyncEngine istruisce esplicitamente risposte in italiano, con linguaggio accessibile. Il Copilot aggiunge un layer conversazionale ulteriore

---

<a id="5-kb"></a>
## 5. AUDIT AREA: KNOWLEDGE BASE

> **Files chiave:**
> - `src/app/api/knowledge/upload/route.ts`
> - `src/app/api/knowledge/scrape/route.ts`
> - `src/lib/interview/manual-knowledge-source.ts`
> - `src/lib/interview/runtime-knowledge.ts`
> - `src/lib/chatbot/knowledge-gap-detector.ts`

### 5.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Knowledge Manager (`/dashboard/bots/[botId]` - sezione KB)
- [‚úì] **Upload file**: Caricamento file (JSON/text) funziona, contenuto salvato in `KnowledgeSource` con tipo, titolo, contenuto
- [‚úì] **Scrape URL**: Lo scraping URL funziona tramite `/api/knowledge/scrape` con estrazione contenuto principale
- [‚úì] **Lista fonti**: Tutte le fonti KB visualizzate nella UI `KnowledgeManager.tsx` con tipo e titolo
- [‚úì] **Elimina fonte**: L'eliminazione rimuove il record `KnowledgeSource` dal DB
- [~] **Preview contenuto**: L'anteprima √® limitata ‚Äî mostra il titolo e tipo, ma **non un'anteprima del contenuto effettivo** per fonti lunghe

### 5.2 Meccanismi di Crescita Automatica üè∑Ô∏è Opus

> **CRITICO**: L'utente richiede crescita automatica da 7 fonti

#### Fonti di crescita attese vs implementate:

| Fonte | Stato Atteso | Stato Reale | Verifica |
|-------|-------------|-------------|----------|
| Trascrizioni chatbot | Auto-grow | **INDIRETTO** | [~] Le conversazioni chatbot alimentano `KnowledgeGap` (non KB direttamente). I gap devono essere approvati manualmente per entrare nella KB |
| Trascrizioni interviste | Auto-grow | **NON IMPLEMENTATO** | [‚úó] Nessuna pipeline estrae contenuto dalle interviste verso la KB. Solo l'auto-interview guide √® generata |
| AI Tips implementate | Auto-grow | **NON IMPLEMENTATO** | [‚úó] I tip implementati (TipAction/CrossChannelInsight.suggestedActions) NON alimentano la KB. Sono due sistemi separati |
| Scraping website | Manuale/Auto | **SOLO MANUALE** | [~] Lo scraping √® solo manuale via `/api/knowledge/scrape`. Nessun re-scraping periodico automatico |
| Dati Analytics (GA4) | Auto-grow | **NON IMPLEMENTATO** | [‚úó] GA4 si sincronizza in `WebsiteAnalytics` (cron cms-sync-analytics) ma NON alimenta la KB |
| Dati GSC | Auto-grow | **NON IMPLEMENTATO** | [‚úó] GSC si sincronizza in `WebsiteAnalytics` ma NON alimenta la KB |
| Knowledge gaps risolti | Auto-grow | **PARZIALE** | [~] Approvare un gap crea `KnowledgeSource` via una route, ma il modello `FaqSuggestion` √® **orfano** (nessuna write path) |

#### Verifica dettagliata:
- [‚úì] **Auto-interview guide**: `ensureAutoInterviewKnowledgeSource()` genera guide automatiche dalla configurazione bot e le salva come `KnowledgeSource` tipo `auto-interview-guide`
- [‚úì] **Runtime knowledge**: `generateRuntimeInterviewKnowledge()` genera intelligence per topic on-the-fly con caching 24h via `RuntimeKnowledgeCache`
- [~] **Gap ‚Üí FAQ pipeline**: La pipeline ha **due code paths**: uno funzionante (crea `KnowledgeSource`), uno che usa `FaqSuggestion` model che √® orfano
- [‚úó] **Chatbot learning loop**: Le conversazioni chatbot creano solo `KnowledgeGap`, non arricchiscono direttamente la KB
- [‚úó] **Interview insight ‚Üí KB**: I temi e citazioni dalle interviste NON alimentano la KB automaticamente
- [‚úó] **Website content sync**: Nessun re-scraping periodico. Solo scraping manuale iniziale
- [‚úó] **Analytics-informed KB**: I dati GSC/GA4 non suggeriscono contenuti KB. Alimentano solo `WebsiteAnalytics` e `CrossChannelSyncEngine`

> **‚ö†Ô∏è FINDING CRITICO**: Solo **2 delle 7 fonti dichiarate** hanno una pipeline funzionante verso la KB (auto-interview guide + knowledge gaps approvati). Le altre 5 sono NON IMPLEMENTATE o solo indirette

### 5.3 Qualit√† KB üè∑Ô∏è Opus

- [~] **Formato contenuto**: Il contenuto KB √® testo raw ‚Äî funzionale per l'LLM ma non strutturato (no Q&A pairs, no metadata semantiche)
- [!] **Rilevanza**: **CRITICO** ‚Äî La selezione fonti KB √® **arbitraria** (nessun ranking per rilevanza semantica). Il chatbot concatena TUTTE le fonti KB nel prompt **senza limite**, rischiando overflow del context window. Per le interviste, solo top 3 per dimensione √ó 260 chars
- [‚úó] **Aggiornamento**: Nessun meccanismo identifica fonti KB obsolete
- [‚úó] **Vector embeddings**: NO ‚Äî solo testo raw. Il codice contiene un TODO per pgvector ma non √® implementato. La ricerca usa keyword matching in `platform-kb.ts`
- [~] **Validazione qualit√†**: Il contenuto auto-generato (auto-interview guide, runtime knowledge) √® generato da LLM ma non c'√® validazione post-generazione. I gap approvati dall'utente hanno validazione implicita (human-in-the-loop)

### 5.4 Gap Implementativi Previsti üè∑Ô∏è Opus

- [‚úó] **Mancanza RAG con embeddings**: CONFERMATO ‚Äî solo testo raw, nessun similarity search. TODO nel codice per pgvector
- [‚úó] **Nessun feedback loop esplicito**: CONFERMATO ‚Äî nessun thumbs up/down su risposte chatbot/KB
- [‚úó] **Nessun versioning**: CONFERMATO ‚Äî le fonti KB non hanno versioning, nessun history delle modifiche
- [~] **Nessuna aggregazione cross-bot**: La KB √® per-bot, MA `platform-kb.ts` offre una KB globale per il copilot. I bot individuali non condividono KB a livello progetto
- [‚úó] **Nessun PII filtering**: CONFERMATO ‚Äî il contenuto che entra nella KB (da scraping, conversazioni, gaps) non viene filtrato per PII. Dati personali degli intervistati possono finire nella KB

---

<a id="6-visibility"></a>
## 6. AUDIT AREA: VISIBILITY & BRAND MONITORING

> **Files chiave:**
> - `src/lib/visibility/serp-monitoring-engine.ts`
> - `src/lib/visibility/visibility-engine.ts`
> - `src/app/api/visibility/` (tutte le route)
> - `src/app/api/cron/serp-monitoring/`

### 6.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Configurazione Brand (`/dashboard/visibility/`)
- [‚úì] **Creazione config**: VERIFICATO ‚Äî `VisibilityConfig` model salva brandName, keywords (JSON array), competitors (JSON array), category, description, languageCode correttamente
- [‚úì] **Prompt generati**: VERIFICATO ‚Äî `generate-prompts/route.ts` usa LLM (temp 0.8) con 6 tipi di variazione (informational, comparative, recommendation, technical, brand-specific, industry-trend). Schema Zod valida output
- [‚úì] **Lista scan**: VERIFICATO ‚Äî `VisibilityScan` model con timestamp, provider results (JSON), summary score. Lo storico √® persistito e queryable
- [‚úì] **Competitors**: VERIFICATO ‚Äî competitors iniettati nel prompt LLM e nel SerpMonitoringEngine per tracking comparativo nelle risposte

#### Risultati Scan (`/dashboard/visibility/[configId]`)
- [‚úì] **Score complessivo**: VERIFICATO ‚Äî formula: `(totalMentions / totalResponses) * 100` calcolata da `VisibilityResult` records
- [‚úì] **Per-provider breakdown**: VERIFICATO ‚Äî `VisibilityEngine` queries 3 provider in parallelo via `Promise.all`: OpenAI (GPT-5.2), Anthropic (Claude Sonnet 4.5), Google Gemini (3.0 Flash). Risultati salvati separatamente per provider
- [‚úì] **AI Overview**: VERIFICATO ‚Äî `SerpMonitoringEngine` usa SerpAPI con `ai_overview` field detection. Fallback: genera query varianti (`generate-prompts`) se AI Overview non presente nella query originale
- [‚úì] **Sentiment per risposta**: VERIFICATO ‚Äî LLM analisi batch (temp 0.1) produce sentiment per ogni risultato SERP. 9 criteri di valutazione
- [‚úì] **Position tracking**: VERIFICATO ‚Äî `VisibilityResult.position` traccia posizione del brand nella risposta LLM (intero, 0-based)
- [‚úì] **Competitor comparison**: VERIFICATO ‚Äî competitors tracciati nelle stesse query. Confronto basato su mention count e sentiment relativo
- [~] **Tips generati**: I suggerimenti SERP includono `suggestedActions` dall'analisi batch LLM, MA **non sono collegati al sistema CMS** per implementazione diretta. Restano puramente informativi

### 6.2 Flussi LLM üè∑Ô∏è Opus

#### SERP Monitoring Engine
- [‚úì] **Query multi-provider**: VERIFICATO ‚Äî `VisibilityEngine` usa `Promise.all` per query parallele a 3 provider (OpenAI GPT-5.2, Anthropic Claude Sonnet 4.5, Gemini 3.0 Flash). Rate limiting: 2s delay tra chiamate SerpAPI
- [‚úì] **Analisi risposte**: VERIFICATO ‚Äî LLM batch analysis (temp 0.1) con 9 criteri: sentiment, relevance, brand mention type, topic category, key claims, source credibility, competitive context, actionable insights, opportunity score
- [‚úì] **Importance scoring**: VERIFICATO ‚Äî Formula 5-dimensionale: 30% source_reputation + 25% relevance + 15% position + 15% mention_type + 15% abs(sentiment). Formula bilanciata e ragionevole per GEO
- [‚úì] **Source reputation**: VERIFICATO ‚Äî Mappa hardcoded di 40+ fonti: Reuters=96, BBC=95, NYT=94, LinkedIn=70, Reddit=50, Medium=55. Copertura adeguata ma **statica** (nessun aggiornamento dinamico delle reputazioni)
- [‚úì] **AI Overview detection**: VERIFICATO ‚Äî SerpAPI `ai_overview` field + generazione query varianti via `generate-prompts` per aumentare probabilit√† di ottenere AI Overview. Fallback robusto

#### Prompt Generation
- [‚úì] **generate-prompts**: VERIFICATO ‚Äî Route `/api/visibility/generate-prompts` usa LLM (temp 0.8) con brand context (name, category, description, language, territory) per generare prompt diversificati
- [~] **refine-prompt**: Il route `refine-prompt` esiste ma √® un **semplice re-generate** con context aggiuntivo, non un vero sistema di raffinamento iterativo
- [‚úì] **Diversit√†**: VERIFICATO ‚Äî 6 tipi di variazione: informational, comparative, recommendation, technical, brand-specific, industry-trend. Copertura buona per GEO monitoring

### 6.3 Allineamento GEO Best Practices üè∑Ô∏è Opus

- [~] **Frequenza scan**: Cron job `serp-monitoring` esiste ma **NON schedulato** (no vercel.json). La frequenza dipende dall'attivazione manuale o da un cron esterno. Per GEO si raccomanda scan settimanale minimo
- [~] **Copertura LLM**: 3 provider (OpenAI, Anthropic, Gemini) coprono i principali AI chatbot. **MANCANO**: Perplexity AI (importante per search-focused queries), Microsoft Copilot/Bing Chat, Meta AI. Copertura ~60% del panorama GEO
- [‚úì] **Metriche GEO**: VERIFICATO ‚Äî Citation rate (mention count/total), brand prominence (position tracking), sentiment trend (sentiment storico per scan). Importance score formula ben calibrata per GEO
- [~] **Actionability**: I suggerimenti SERP sono informativi ma **non collegati a CMS/automation**. Manca il bridge suggerimento‚Üíazione. L'utente deve manualmente implementare ogni suggerimento
- [‚úì] **Benchmark competitor**: VERIFICATO ‚Äî Competitor tracking persistente con confronto mention count, sentiment, position nel tempo. Dashboard mostra trend comparativi

---

<a id="7-cms"></a>
## 7. AUDIT AREA: CMS & CONTENT SUGGESTIONS

> **Files chiave:**
> - `src/lib/cms/suggestion-generator.ts` (CMSSuggestionGenerator)
> - `src/lib/cms/mcp-client.ts` (connessione MCP WordPress)
> - `src/app/api/cms/` (tutte le route)
> - `src/app/api/cron/cms-generate-suggestions/`
> - `src/app/api/cron/cms-sync-analytics/`

### 7.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Dashboard CMS (`/dashboard/cms`)
- [‚úì] **Stato connessione**: VERIFICATO ‚Äî `CMSConnection` model con status enum (CONNECTED, DISCONNECTED, ERROR). Verificato via `test-connection` API route
- [‚úì] **Suggerimenti contenuto**: VERIFICATO ‚Äî `CMSSuggestion` model persistito con tutti i campi. Query da `cms/suggestions` route con filtri per status/type
- [‚úì] **Tipo contenuto**: VERIFICATO ‚Äî 5 tipi distinti: CREATE_FAQ, CREATE_BLOG_POST, CREATE_PAGE, MODIFY_CONTENT, ADD_SECTION. Ognuno con schema SEO specifico
- [‚úì] **Priority score**: VERIFICATO ‚Äî Score 0-100 calcolato da LLM (temp 0.25) basato su: relevance strategica, volume di ricerca implicito, gap contenutistico, urgency
- [‚úì] **Reasoning**: VERIFICATO ‚Äî Campo `reasoning` obbligatorio nello schema Zod. LLM deve fornire motivazione basata su dati fonte (insight, visibility, chatbot gaps)
- [‚úì] **SEO data**: VERIFICATO ‚Äî Schema completo: metaTitle, metaDescription, focusKeyword, slug, schemaOrgJsonLd (FAQ/BlogPosting/Product/WebPage), categories, tags. WooCommerce attrs per prodotti
- [‚úì] **Push to CMS**: VERIFICATO ‚Äî Route `cms/push` chiama `MCPClient` per WordPress o `CMS API` per Voler.ai. Workflow funzionale end-to-end
- [‚úì] **Status workflow**: VERIFICATO ‚Äî PENDING ‚Üí PUSHED ‚Üí PUBLISHED (o REJECTED). Ogni transizione loggate con timestamp

#### Connessione WordPress
- [‚úì] **Setup flow**: VERIFICATO ‚Äî `MCPClient` (mcp-client.ts) gestisce connessione WordPress via MCP protocol. Config persistita in `CMSConnection`
- [‚úì] **Capabilities detection**: VERIFICATO ‚Äî `SiteDiscoveryService` rileva: hasWooCommerce, hasBlog, hasPages, hasProducts tramite query MCP+API
- [‚úì] **Site structure discovery**: VERIFICATO ‚Äî Scoperta multi-source: WordPress MCP, WooCommerce MCP, Voler.ai CMS API. Cache 24h TTL per ridurre chiamate
- [‚úì] **Test connection**: VERIFICATO ‚Äî Route `cms/test-connection` verifica raggiungibilit√† effettiva. Ritorna capabilities e errori specifici

### 7.2 Flussi LLM üè∑Ô∏è Opus

#### Content Generation
- [‚úì] **Prompt qualit√†**: VERIFICATO ‚Äî Prompt include: brand context (name, sector, description), strategic plan, insight signals, site structure (categories, tags, pages, posts, products). 15 istruzioni numerate nel prompt
- [~] **Content quality**: Qualit√† LLM buona (temp 0.25) con schema Zod rigoroso, MA **nessuna review umana obbligatoria** prima della pubblicazione. Il contenuto pu√≤ essere pushato direttamente
- [‚úì] **SEO optimization**: VERIFICATO ‚Äî heading structure (H1/H2/H3), meta title/description, focus keyword, slug SEO-friendly. Schema.org JSON-LD per ogni tipo contenuto
- [‚úì] **Schema.org**: VERIFICATO ‚Äî 4 tipi: FAQPage (per FAQ), BlogPosting (per blog), Product (per WooCommerce), WebPage (generico). JSON-LD valido generato dal LLM con validazione schema
- [‚úì] **Slug generation**: VERIFICATO ‚Äî Slug generato dal LLM con istruzione "slug SEO-friendly, lowercase, hyphenated". Unicit√† non verificata programmaticamente (rischio duplicati)
- [~] **Tone alignment**: Il prompt include brand context e strategic plan per allineamento tono, MA **solo in italiano** (P28). Nessun variant inglese per brand internazionali

#### Allineamento Digital Marketing B2B üè∑Ô∏è Opus
- [‚úì] **Tipi contenuto B2B**: VERIFICATO ‚Äî 5 tipi coprono: FAQ (customer education), Blog Post (thought leadership), Page (landing/service), Modify Content (optimization), Add Section (enhancement). Adatti a PMI B2B
- [~] **Topic cluster strategy**: I suggerimenti sono basati su **signal-driven strategy** (insight‚Üícontent) non su topic cluster espliciti. Buono per reattivit√†, debole per strategia SEO pillar/cluster
- [~] **Keyword strategy**: Le keyword provengono dai dati GSC **solo se integrazione Google Analytics attiva**. Altrimenti keyword inferite dal LLM (meno accurate)
- [‚úó] **Content calendar**: NON implementato ‚Äî nessuna logica di distribuzione temporale. I suggerimenti sono one-shot, non pianificati nel tempo
- [‚úó] **Multi-format**: NON implementato ‚Äî solo contenuti testuali. Nessun suggerimento immagini, video, infografiche

### 7.3 Integrazioni Mancanti üè∑Ô∏è Opus

- [‚úì] **Pubblicazione automatica**: VERIFICATO ‚Äî Flusso end-to-end funzionale: `CMSSuggestion` ‚Üí `MCPClient.push()` ‚Üí WordPress/WooCommerce/Voler.ai CMS. Status aggiornato automaticamente
- [‚úó] **Modifica pre-pubblicazione**: NON implementato ‚Äî Il contenuto viene pushato cos√¨ com'√® generato dall'LLM. Nessun editor inline per modifiche pre-push
- [‚úì] **Selezione destinazione**: VERIFICATO ‚Äî Il tipo contenuto (blog, FAQ, page) determina la destinazione. Per WooCommerce, i prodotti vanno al catalogo
- [‚úó] **Performance tracking**: NON implementato ‚Äî Dopo la pubblicazione, nessun tracking di views, bounce rate, conversioni. Il cron `cms-sync-analytics` esiste ma **NON schedulato** (no vercel.json)
- [‚úó] **A/B testing**: NON implementato ‚Äî Nessun supporto A/B testing
- [~] **Multi-CMS**: PARZIALE ‚Äî Supporto WordPress (via MCP) e Voler.ai CMS API. Nessun altro CMS (Shopify, Webflow, etc.)
- [‚úó] **Social publishing**: NON implementato ‚Äî Nessuna integrazione social diretta. Solo webhook n8n pianificato ma disconnesso

---

<a id="8-tips"></a>
## 8. AUDIT AREA: AI TIPS & AUTOMAZIONE

> **Files chiave:**
> - `src/lib/insights/sync-engine.ts` (generazione suggestedActions)
> - Modello `TipAction` nel Prisma schema
> - UI componenti per visualizzazione tips

### 8.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Visualizzazione Tips
- [~] **Lista tips**: Esistono **DUE sistemi separati**: `TipAction` model (Brand Monitor) e `CrossChannelInsight.suggestedActions` (Insights Hub). `InsightCard.tsx` √® un componente **obsoleto non usato**. I tip su `/dashboard/insights` provengono da CrossChannelInsight
- [‚úì] **Tipi azione**: I tipi nell'InsightsHub includono 11 action types distinti (add_faq, create_blog, create_social_post, update_kb, respond_to_press, etc.)
- [‚úì] **Praticit√†**: Ogni tip include target, titolo, body, reasoning e urgency dal CrossChannelSyncEngine
- [!] **Modificabilit√†**: **CRITICO** ‚Äî I tip NON sono modificabili prima dell'implementazione. Non c'√® editor inline n√© preview
- [~] **Canale target**: Il canale √® indicato nel tipo azione ma **non linkato** a una connessione specifica
- [!] **Implementazione diretta**: Il pulsante **"Applica"** sulla pagina insights **NON ha onClick handler** ‚Äî √® un bottone puramente decorativo. Nessuna azione viene eseguita al click

### 8.2 Workflow di Implementazione üè∑Ô∏è Opus

- [‚úó] **Manual ‚Üí Automated**: NON esiste un flusso automatizzato. L'utente deve implementare manualmente ogni suggerimento
- [‚úó] **Confirmation flow**: Nessun workflow edit ‚Üí preview ‚Üí approve. Il bottone "Applica" √® decorativo
- [~] **Multi-channel routing**: Il `n8n/dispatcher.ts` esiste per routing webhook-based, MA √® disconnesso dal flusso insight. Il dispatcher non viene chiamato quando un tip viene "applicato"
- [‚úó] **Feedback loop**: Nessun feedback post-implementazione. I tip non alimentano la KB
- [‚úó] **Scheduling**: NON implementato ‚Äî nessuna schedulazione futura
- [‚úó] **Batch operations**: NON implementato ‚Äî nessuna operazione batch

### 8.3 Gap Implementativi Previsti üè∑Ô∏è Opus

- [‚úó] **Social media integration**: CONFERMATO ‚Äî nessuna integrazione diretta. Solo webhook n8n pianificato
- [‚úó] **Email marketing**: CONFERMATO ‚Äî nessuna integrazione con piattaforme email
- [~] **Automazione n8n**: Il model `N8NConnection` esiste nel DB con `webhookUrl`, `apiKey`, `activeWorkflows`. Il dispatcher `n8n/dispatcher.ts` ha logica di routing per tipo azione, MA **non √® connesso al flusso di applicazione tip**. L'integrazione √® strutturalmente presente ma funzionalmente disconnessa
- [‚úó] **Approval workflow**: CONFERMATO ‚Äî nessun workflow multi-step. Solo stato insight (pending ‚Üí actioned ‚Üí completed ‚Üí archived) senza step intermedi
- [‚úó] **Template per canale**: CONFERMATO ‚Äî nessun template specifico per canale. I chatbot hanno template (`chatbot-templates.ts`) ma non i tip/contenuti social

---

<a id="9-copilot"></a>
## 9. AUDIT AREA: COPILOT STRATEGICO

> **Files chiave:**
> - `src/lib/copilot/system-prompt.ts`
> - `src/lib/copilot/chat-tools.ts`
> - `src/app/api/copilot/chat/route.ts`

### 9.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Finestra Copilot (`StrategyCopilot` component)
- [‚úì] **Attivazione**: VERIFICATO ‚Äî `StrategyCopilot.tsx` implementa floating chat button bottom-right con z-index management. Montato nei layout wrappers, accessibile da tutte le pagine dashboard
- [‚úì] **Contesto progetto**: VERIFICATO ‚Äî `buildProjectContext()` carica: botCount, conversationCount (30gg), topThemes (top 5), avgSentiment, strategicVision, valueProposition dal progetto corrente
- [‚úì] **Piano strategico**: VERIFICATO ‚Äî Recuperato da `platformSettings.strategicPlan`. Iniettato nel system prompt SOLO per tier PRO+ (PRO, BUSINESS, ENTERPRISE, ADMIN, PARTNER)
- [~] **Citazione fonti**: Il prompt include istruzione anti-hallucination ("Se non hai abbastanza dati, dillo chiaramente") e la risposta include flag `usedKnowledgeBase`. MA **non cita fonti specifiche** (es. "da intervista X" o "da scan visibility Y")
- [‚úì] **Tier limitation**: VERIFICATO ‚Äî `canAccessProjectData(tier)` in `permissions.ts` limita tools e accesso dati a PRO+. Quick actions differenziati per tier. Basic vede solo help generico

### 9.2 Flussi LLM üè∑Ô∏è Opus

#### System Prompt Copilot
- [‚úì] **Lingua italiana**: VERIFICATO ‚Äî `system-prompt.ts` √® interamente in italiano. Istruzioni, formatting rules, e quick actions in italiano. **GAP**: nessun variant inglese per utenti internazionali
- [‚úì] **Contesto organizzazione**: VERIFICATO ‚Äî Nome org, piano, tier iniettati nel system prompt builder. KB platform (49 entries) iniettata come contesto aggiuntivo
- [‚úì] **Dati progetto**: VERIFICATO ‚Äî Bot count, conversation count (30gg), sentiment avg, top 5 temi con sentiment per tema. Dati aggiornati ad ogni richiesta
- [‚úì] **Tools disponibili**: VERIFICATO ‚Äî 7 tools implementati in `chat-tools.ts`: getProjectTranscripts, getChatbotConversations, getProjectIntegrations, getVisibilityInsights, getExternalAnalytics, getKnowledgeBase, scrapeWebSource. Rate limit: 5/query (max 10). maxSteps: 3
- [~] **Qualit√† consigli**: Il prompt √® orientato a PMI italiane con strategic plan injection, MA la qualit√† dipende dalla ricchezza dei dati progetto. Con pochi dati, i consigli tendono a essere generici
- [‚úì] **Non inventa dati**: VERIFICATO ‚Äî Istruzione esplicita nel prompt: "Se non hai abbastanza dati, dillo chiaramente invece di inventare"
- [‚úì] **Visione d'insieme**: VERIFICATO ‚Äî Tool chain permette al copilot di accedere a interviste + chatbot + visibility + analytics + KB. `generateText` con maxSteps:3 permette multi-tool calls in una singola risposta

### 9.3 Obiettivo Business üè∑Ô∏è Opus

- [‚úì] **Supporto decisionale**: VERIFICATO ‚Äî Strategic plan + project data + multi-source tools permettono raccomandazioni data-driven. Dual LLM fallback (Anthropic primary, OpenAI secondary) garantisce disponibilit√†
- [‚úì] **Accessibilit√† linguistica**: VERIFICATO ‚Äî Prompt include istruzioni di formatting markdown con bullet points, emojis, e linguaggio chiaro. Orientato a management non-tecnico
- [~] **Proattivit√†**: Il copilot √® REATTIVO (risponde a domande), NON proattivo. Non invia notifiche o suggerimenti unsolicited. `suggestedFollowUp` nella risposta √® l'unica forma di proattivit√†
- [‚úì] **Limiti riconosciuti**: VERIFICATO ‚Äî Istruzione anti-hallucination nel prompt. Tier limitation comunicata esplicitamente agli utenti Basic

---

<a id="10-billing"></a>
## 10. AUDIT AREA: BILLING & CREDITS

> **Files chiave:**
> - `src/lib/stripe.ts`
> - `src/config/creditCosts.ts`
> - `src/services/creditService.ts`
> - `src/services/tokenTrackingService.ts`
> - `src/app/api/stripe/` (webhook, checkout, portal)
> - `src/app/api/credits/`

### 10.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Settings Billing (`/dashboard/settings/billing`)
- [‚úì] **Piano corrente**: VERIFICATO ‚Äî `Subscription` model sincronizzato con Stripe via webhook. Piano mostrato da `organization.plan` aggiornato da `handleSubscriptionUpdated()`
- [‚úì] **Limiti piano**: VERIFICATO ‚Äî `PLANS` config in `plans.ts` definisce limiti per tier. `monthlyCreditsLimit` aggiornato da webhook on subscription change
- [‚úì] **Upgrade/downgrade**: VERIFICATO ‚Äî Route `stripe/checkout` con proration handling: upgrade=`always_invoice` (charge immediato), downgrade=`create_prorations` (credit su fatture future). `billing_cycle_anchor: unchanged`
- [‚úì] **Fatture**: VERIFICATO ‚Äî Gestione invoice via webhook events `invoice.paid`, `invoice.payment_succeeded`, `invoice.payment_failed`

#### Credits (`/dashboard/settings/credits`)
- [‚úì] **Saldo crediti**: VERIFICATO ‚Äî Calcolo: `monthlyCreditsLimit - monthlyCreditsUsed + packCreditsAvailable`. `-1` = unlimited. Consumo tramite `CreditService.consumeCredits()`
- [‚úì] **Storico transazioni**: VERIFICATO ‚Äî `orgCreditTransaction` table registra ogni operazione con: amount, action, tool, executedBy, timestamp. Audit trail completo
- [‚úì] **Costi per azione**: VERIFICATO ‚Äî 13 azioni con costi fissi in `creditCosts.ts`: interview_question=8, chatbot_message=3, visibility_query=6, copilot_message=20, copilot_analysis=35, export_pdf_analysis=30, etc.
- [~] **Alert crediti bassi**: L'infrastruttura per alert esiste (credit tracking) MA **nessun alert proattivo UI** a 80%/95%. L'utente scopre l'esaurimento solo quando riceve errore 429

### 10.2 Pipeline Dati üè∑Ô∏è Haiku

- [‚úì] **Token ‚Üí Crediti**: VERIFICATO ‚Äî `TOKEN_TO_CREDIT_RATE = 0.0005`. Formula: `creditsToConsume = max(modelAdjustedBaseCost, ceil(totalTokens * RATE * multiplier))`
- [‚úì] **Model multiplier**: VERIFICATO ‚Äî Moltiplicatori per modello: small models (haiku,mini,flash-8b)=1.0x, GPT-4o/Claude Sonnet=~3.0x, Claude Opus=~8.0x (capped)
- [~] **Cron reset mensile**: Route `reset-credits` esiste con logica corretta (reset `monthlyCreditsUsed=0`, calcola `nextResetDate`), MA **NON schedulato** (no vercel.json). Deve essere attivato da cron esterno
- [~] **Credit pack expiry**: I pack sono persistiti in `orgCreditPack` con `packCreditsAvailable`, MA **nessuna scadenza esplicita** ‚Äî i pack non scadono mai. Non √® un bug ma una scelta di business
- [‚úì] **Webhook Stripe**: VERIFICATO ‚Äî 5 eventi gestiti: `checkout.session.completed` (pack+subscription), `customer.subscription.updated`, `customer.subscription.deleted` (downgrade a TRIAL), `invoice.paid`/`payment_succeeded` (reset cycle), `invoice.payment_failed` (PAST_DUE). Deduplicazione via `stripeWebhookEvent` unique constraint
- [‚úì] **Double-charge prevention**: VERIFICATO ‚Äî Multi-layer: (1) webhook dedup via `stripeWebhookEvent`, (2) credit tracking a livello org (single counter), (3) transaction logging, (4) idempotent `addPackCredits()`

---

<a id="11-dashboard"></a>
## 11. AUDIT AREA: DASHBOARD & SETTINGS

### 11.1 UI ‚Üî Codice üè∑Ô∏è Sonnet

#### Dashboard Principale (`/dashboard`)
- [‚úì] **Progetti**: VERIFICATO ‚Äî Query `project.findMany` filtrata per `organizationId` da membership attiva. Mostra tutti i progetti dell'organizzazione corrente
- [‚úì] **Quick stats**: VERIFICATO ‚Äî Metriche calcolate da DB: bot count, conversation count, lead count. Periodo 30gg per trend
- [‚úì] **Empty state**: VERIFICATO ‚Äî Stato vuoto con CTA per creare primo progetto. Guidato per tier
- [‚úì] **Navigazione**: VERIFICATO ‚Äî Sidebar navigation condizionata per: user role (ADMIN/EDITOR/VIEWER), subscription tier, feature flags (`canPublishBot`, `canCreateChatbot`, `canAccessCopilot`, `canAccessVisibilityTracker`)

#### Settings Organizzazione (`/dashboard/settings`)
- [‚úì] **Info organizzazione**: VERIFICATO ‚Äî `PlatformSettingsState` con form validation, save debounced. Solo org admin pu√≤ modificare
- [‚úì] **Piano strategico**: VERIFICATO ‚Äî `strategicPlan` (rich text, 30k chars max) persistito in `platformSettings`. Usato da CrossChannelSyncEngine (sync-engine.ts) E da Copilot (system-prompt.ts) per contesto strategico
- [‚úì] **Membri team**: VERIFICATO ‚Äî `Membership` model con ruoli ADMIN, EDITOR, VIEWER e status ACTIVE/INVITED/INACTIVE. `ProjectAccess` per accesso fine-grained per progetto
- [‚úì] **API keys**: VERIFICATO ‚Äî `GlobalConfig` model salva API keys (Stripe, OpenAI, Anthropic, Google Analytics, SMTP). Show/hide toggles. Keys usate con fallback a env vars
- [‚úì] **Connessioni**: VERIFICATO ‚Äî Stato CMS via `CMSConnection`, Google via `GoogleAnalyticsIntegration`, n8n via `N8NConnection`

#### Admin (`/dashboard/admin`)
- [‚úì] **Solo admin**: VERIFICATO ‚Äî Server-side check esplicito: `if (session?.user?.role !== 'ADMIN') redirect('/dashboard')`. Non bypassabile client-side
- [‚úì] **Stats piattaforma**: VERIFICATO ‚Äî Admin pu√≤ visualizzare tutte le organizzazioni, membri, progetti, bot count, tool usage
- [‚úì] **Gestione organizzazioni**: VERIFICATO ‚Äî Admin accede a lista org con dettagli membri, ruoli, status, joinedAt
- [~] **Gestione utenti**: Admin panel **semplicistico** ‚Äî visualizza dati ma la modifica limiti √® limitata. Nessun bulk operations o advanced user management

---

<a id="12-prompts"></a>
## 12. AUDIT CROSS-CUTTING: QUALIT√Ä PROMPT

> üè∑Ô∏è **Modello:** Opus per tutti i task di questa sezione

### 12.1 Inventario Completo Prompt

| # | Prompt | File | Modello | Temp | Score | Verifica |
|---|--------|------|---------|------|-------|----------|
| 1 | Interview Identity Block | prompt-builder.ts | gpt-4o | 0.7 | 8/10 | [‚úì] Bilingue (IT/EN), buon ruolo |
| 2 | Interview Context Block | prompt-builder.ts | gpt-4o | 0.7 | 8/10 | [‚úì] Schema completo, turn-aware |
| 3 | Interview Topic Focus | prompt-builder.ts | gpt-4o | 0.7 | 7/10 | [‚úì] Dinamico per topic, coverage tracking |
| 4 | Interview Memory Block | prompt-builder.ts | gpt-4o | 0.7 | 8/10 | [‚úì] Last 5 turns, summary compatto |
| 5 | Interview Knowledge Block | prompt-builder.ts | gpt-4o | 0.7 | 7/10 | [!] **BUG**: `isItalian = label.length > 0` sempre true ‚Üí forza IT per tutte le lingue |
| 6 | Turn Guidance Block | runtime-prompt-blocks.ts | gpt-4o | 0.7 | 8/10 | [‚úì] Signal-driven, depth-aware strategy |
| 7 | Guards Block | runtime-prompt-blocks.ts | gpt-4o | 0.7 | 7/10 | [‚úì] Multi-guard, condizionale |
| 8 | Supervisor Runtime Banner | chat/route.ts | gpt-4o | 0.7 | 7/10 | [‚úì] State-dependent, inline injection |
| 9 | Runtime Semantic Context | chat/route.ts | gpt-4o | 0.7 | 9/10 | [‚úì] Eccellente bridge stem dedup, anti-formula |
| 10 | Micro-Planner Block | micro-planner.ts | N/A (determ.) | - | 8/10 | [‚úì] Deterministico, no LLM call |
| 11 | Tone Adaptation | tone-prompt-adapter.ts | gpt-4o | 0.7 | 5/10 | [!] **CRITICO**: Solo italiano ‚Äî nessun variant EN |
| 12 | Validation Feedback | prompt-builder.ts | gpt-4o | 0.7 | 7/10 | [~] Condizionale, feedback non escaped |
| 13 | Question-Only Generation | chat/route.ts | gpt-4o | 0.2 | 8/10 | [‚úì] Anti-repetition, bridge stems |
| 14 | Deep Offer Generation | chat/route.ts | gpt-4o | 0.2 | 7/10 | [‚úì] 4-step structure, constraints chiari |
| 15 | Deep Offer Enforcement | chat/route.ts | gpt-4o | 0.3 | 7/10 | [‚úì] Enforcement post-generation |
| 16 | Consent Question | chat/route.ts | gpt-4o | 0.2 | 7/10 | [~] Manca framing GDPR per contesto EU |
| 17 | User Intent Classification | chat/route.ts | gpt-4o-mini | 0.0 | 7/10 | [~] Hybrid determ+LLM, fast-path solo IT |
| 18 | Closure Intent Detection | chat/route.ts | gpt-4o-mini | 0.0 | 8/10 | [‚úì] Ottima prevenzione false-positive |
| 19 | Field Extraction | chat/route.ts | gpt-4o-mini | 0.0 | 7/10 | [!] **DUPLICATO** in chatbot/message/route.ts |
| 20 | Candidate Profile Extraction | candidate-extractor.ts | gpt-4o | ‚ö†Ô∏è MANCA | 6/10 | [!] **Temp non impostata** (default ~1.0 per extraction!) |
| 21 | Chatbot System Prompt | chatbot/message/route.ts | gpt-4o-mini | ‚ö†Ô∏è MANCA | 7/10 | [~] KB non sanitizzata, temp mancante |
| 22 | Smart Lead Capture | chatbot/message/route.ts | gpt-4o-mini | ‚ö†Ô∏è MANCA | 6/10 | [!] Temp mancante per classificazione, prompt minimal |
| 23 | Copilot System Prompt | copilot/system-prompt.ts | configurable | 0.3 | 7/10 | [~] Solo italiano, buon anti-hallucination |
| 24 | Runtime Knowledge Gen. | runtime-knowledge.ts | gpt-4o | 0.25 | 8/10 | [‚úì] Fallback robusto, caching, schema strict |
| 25 | Knowledge Gap Analysis | knowledge-gap-detector.ts | gpt-4o-mini | ‚ö†Ô∏è MANCA | 4/10 | [!] No role def, no lang, no format guidance |
| 26 | Analytics Aggregation | analytics-aggregator.ts | gpt-4o | ‚ö†Ô∏è MANCA | 4/10 | [!] gpt-4o troppo costoso, key hardcoded, no lang |
| 27 | CrossChannel Insight | sync-engine.ts | systemLLM | 0.15 | 7/10 | [~] Solo IT, prompt molto lungo (~2000tok) |
| 28 | CMS Content Suggestion | suggestion-generator.ts | systemLLM | 0.25 | 7/10 | [~] Solo IT, 15 istruzioni, buon SEO schema |
| 29 | SERP Response Analysis | serp-monitoring-engine.ts | systemLLM | 0.1 | 7/10 | [!] **RISCHIO INJECTION**: dati SERP esterni non sanitizzati |
| 30 | Visibility Prompt Gen. | visibility/generate-prompts | systemLLM | 0.8 | 6/10 | [!] Temp troppo alta, input utente non sanitizzato |

**Score medio complessivo: 6.9/10**
- Interview Core (P1-P9): **7.7/10**
- Interview Planning (P10-P12): **7.0/10**
- Interview Enforcement (P13-P19): **7.3/10**
- Extraction/Analytics (P20, P25, P26): **4.7/10** ‚ö†Ô∏è
- Chatbot (P21-P22): **6.5/10**
- Copilot (P23): **7.0/10**
- Knowledge/Insights (P24, P27): **7.5/10**
- CMS/Visibility (P28-P30): **6.7/10**

### 12.2 Criteri di Valutazione Prompt ‚Äî RISULTATI

Per ogni prompt verificato:

- [‚úì] **Chiarezza ruolo**: 24/30 prompt hanno ruolo chiaro. P25, P26 mancano "You are..." preamble
- [‚úì] **Contesto sufficiente**: 26/30 prompt hanno contesto sufficiente. P16, P22, P25 sono minimali
- [~] **Guardrail efficaci**: Interview core eccellente (multi-layer enforcement). Chatbot/CMS/Visibility meno rigorosi
- [‚úì] **Output format**: 28/30 usano Zod schema per structured output. Buona pratica
- [!] **Lingua coerente**: **GAP CRITICO** ‚Äî 5 prompt solo italiano (P11, P23, P27, P28, P29) senza variant EN. P5 ha bug `isItalian` che forza italiano su tutte le lingue
- [~] **No conflitti**: Nessun conflitto diretto trovato tra blocchi. Possibile tensione tra P6 (turn guidance) e P7 (guards) in edge cases
- [!] **Temperatura appropriata**: **5 prompt senza temperatura esplicita** (P20, P21, P22, P25, P26). P20 (extraction) particolarmente critico con default ~1.0
- [‚úì] **Modello appropriato**: Generalmente buono. P26 usa gpt-4o (costoso) dove gpt-4o-mini basterebbe
- [~] **Fallback**: Interview core ha fallback robusti. Chatbot/CMS/Visibility hanno gestione errori ma non fallback di contenuto
- [~] **Token efficiency**: Interview system prompt composito (7 blocchi) √® ottimizzato con injection condizionale. P27 (insight) √® il pi√π costoso (~2000 tokens prompt)

### 12.3 Pattern Problematici ‚Äî RISULTATI

- [!] **Prompt injection vulnerability**: **CRITICO** ‚Äî Input utente iniettato senza sanitizzazione in P9, P13, P17, P18, P21, P25, P26, P29, P30. Quote escaping mancante in P13, P17, P18. **P29 (SERP data) √® il vettore pi√π pericoloso** perch√© inietta dati da fonti esterne (Google SERP)
- [~] **System prompt leak**: Chatbot ha `PROMPT_LEAK_PATTERNS` detection su input utente (buono), MA **nessun check output-side** (il modello potrebbe comunque leakare frammenti)
- [‚úì] **Hallucination risk**: Interview core e Copilot hanno istruzioni anti-hallucination esplicite. Chatbot ha scope guardrails ("STRICT SCOPE")
- [~] **Context window overflow**: Interview system prompt (7 blocchi + history) pu√≤ crescere significativamente. `buildProjectContext` limita a 50 conversations (ridotto da 100). Transcript non troncato in P20
- [~] **Conflitto istruzioni**: Nessun conflitto diretto. Tensione potenziale tra supervisor banner (P8) e micro-planner (P10) su strategia
- [~] **Degradazione qualit√†**: Memory block (P4) limita a 5 turns recenti. Runtime knowledge cached. Qualit√† stabile ma **context pressure** aumenta con conversazioni lunghe

#### TOP 5 FIX PRIORITARI PROMPT

1. **P5 Language Bug** ‚Äî `isItalian = label.length > 0` ‚Üí fix: `(bot.language || 'en').startsWith('it')` ‚Äî **1 linea, impatto alto**
2. **P20 Missing Temperature** ‚Äî Aggiungere `temperature: 0` a `generateObject()` in candidate-extractor.ts ‚Äî **1 linea, impatto medio**
3. **P29 SERP Injection** ‚Äî Creare `sanitizeSerpContent()` per sanitizzare dati SERP esterni ‚Äî **architetturale, impatto sicurezza**
4. **P30 User Input Injection** ‚Äî Sanitizzare `brandName`, `category`, `description` prima di injection in prompt ‚Äî **API security**
5. **P11,P23,P27,P28,P29 Italian-Only** ‚Äî Aggiungere variant EN per i 5 prompt Italian-only ‚Äî **i18n gap significativo**

---

<a id="13-lingua"></a>
## 13. AUDIT CROSS-CUTTING: LINGUA E LOCALIZZAZIONE

> üè∑Ô∏è **Modello:** Sonnet

### 13.1 Interfaccia Utente (Italiano)

- [~] **Dashboard labels**: Dashboard √® Italian-first con stringhe hardcoded. La maggior parte dei label e menu sono in italiano, MA **nessun framework i18n** (no next-intl, no react-i18n). Stringhe non-translatable
- [~] **Error messages**: Mix italiano/inglese. Esempi IT: "Non autorizzato", "Crediti insufficienti", "Piano non acquistabile". Alcuni errori tecnici restano in inglese
- [~] **Empty states**: Stati vuoti in italiano con guida, MA qualit√† variabile. Alcuni CTA sono generici
- [~] **Tooltip/help**: Copilot platform KB (49 entries) in italiano. Tooltip UI parzialmente presenti
- [‚úó] **Email transazionali**: **NON VERIFICABILE** ‚Äî Nessuna directory `/src/emails` o template email trovati nel codebase. Probabilmente gestiti da servizio esterno (SendGrid/Mailgun). Completezza italiano non auditabile
- [~] **Pricing page**: Pricing in EUR tramite Stripe. Descrizioni piani in italiano nel codebase (`plans.ts`)

### 13.2 Tool Raccolta Dati (Multilingua)

- [‚úì] **Interview widget**: VERIFICATO ‚Äî 5 lingue supportate: Italiano, English, Espa√±ol, Fran√ßais, Deutsch. Parametro `language` nell'embed script (`?language=italiano`)
- [‚úì] **Chatbot widget**: VERIFICATO ‚Äî Lingua configurabile per bot via `languageCode` field. Chatbot system prompt adattato alla lingua
- [‚úì] **Data collection labels**: VERIFICATO ‚Äî `getFieldLabel()` con mappa i18n per campi standard (nome, email, telefono, azienda) in IT/EN/ES/FR/DE
- [‚úì] **Consent text**: VERIFICATO ‚Äî Testo consenso parametrizzato per lingua via `generateConsentQuestionOnly()`
- [‚úì] **Completion message**: VERIFICATO ‚Äî Messaggio finale nella lingua configurata del bot

### 13.3 Prompt LLM

- [‚úì] **Identity block**: VERIFICATO ‚Äî P1 (Identity Block) bilingue con flag `isItalian` che determina lingua prompt. **CAVEAT**: vedi bug P5 dove `isItalian` √® sempre true
- [‚úì] **Supervisor banner**: VERIFICATO ‚Äî P8 (Supervisor Banner) parametrizzato per lingua. Banner runtime nella lingua corretta
- [‚úì] **Field labels**: VERIFICATO ‚Äî `buildTopicFocusBlock()` usa mappa i18n per field labels
- [~] **Fallback language**: Se lingua non supportata, il sistema degrada a **inglese** per interview/chatbot. MA 5 prompt ausiliari (P11,P23,P27,P28,P29) sono solo italiano senza fallback
- [‚úì] **Copilot**: VERIFICATO ‚Äî Copilot risponde sempre in italiano (prompt Italian-only). **GAP**: utenti internazionali ricevono istruzioni italiane nel copilot

---

<a id="14-sicurezza"></a>
## 14. AUDIT CROSS-CUTTING: SICUREZZA E PRIVACY

> üè∑Ô∏è **Modello:** Sonnet + Opus per analisi profonda

### 14.1 Autenticazione & Autorizzazione

- [‚úì] **Password hashing**: bcryptjs con salt rounds adeguato in `auth.ts`
- [~] **JWT security**: JWT sessions con NextAuth v5. Il secret dipende da `NEXTAUTH_SECRET` env var ‚Äî **da verificare che sia sufficientemente forte in produzione**
- [~] **Role enforcement**: I ruoli sono definiti ma **le pagine admin non sono protette dal middleware**. Il `middleware.ts` controlla solo `/dashboard` generico, non `/dashboard/admin/*` specificamente. Il controllo avviene a livello di componente/route individuale
- [‚úì] **Project access**: `ProjectAccess` √® verificato tramite `organizationId` match nelle query Prisma
- [~] **Organization isolation**: Generalmente isolata tramite `organizationId` nelle query, MA **la route `/api/knowledge/gaps` NON filtra per organizzazione** ‚Äî possibile cross-org leak

> **‚ö†Ô∏è FINDING CRITICI ‚Äî Cron Job Security:**
>
> | Cron | Auth Status | Problema |
> |------|------------|----------|
> | `detect-gaps` | **AUTH COMMENTATA** | Chiunque pu√≤ triggherare gap detection per TUTTI i bot PUBLISHED |
> | `sync-insights` | **ZERO AUTH** | Nessun check di autenticazione. Processa TUTTE le organizzazioni |
> | `cms-generate-suggestions` | Condizionale | `if (process.env.CRON_SECRET)` ‚Äî skip auth se env var non settata |
> | `serp-monitoring` | Condizionale | Stessa logica condizionale |
> | `cms-sync-analytics` | Condizionale | Stessa logica condizionale |
> | `interview-quality-alerts` | Condizionale | Stessa logica condizionale |
> | `aggregate-chatbot-analytics` | ‚úì Bearer | Corretto: `Authorization: Bearer` |
> | `reset-monthly-counters` | ‚úì Bearer | Corretto: `Authorization: Bearer` |
> | `reset-credits` | ‚úì x-cron-secret | Header diverso: `x-cron-secret` |
> | `partner-fees` | ‚úì x-cron-secret | Header diverso: `x-cron-secret` |
>
> **Header inconsistency**: 3 diversi metodi di auth (Bearer, x-cron-secret, condizionale) tra 10 cron jobs
>
> **‚ö†Ô∏è CRITICO: Nessun `vercel.json` trovato** ‚Äî NESSUN cron job √® schedulato su Vercel. Tutta l'automazione √® inattiva

### 14.2 Privacy Dati

- [!] **PII in logs**: **CRITICO** ‚Äî Multipli `console.log` con dati utente non redatti: user message preview (chat/route.ts:2013), bot response (chat/route.ts:3498,3514), profile data (chat/route.ts:1770), field extraction failures (chatbot/message/route.ts:223). Nomi, email, telefoni potenzialmente esposti nei server logs
- [~] **Consenso GDPR**: PARZIALE ‚Äî `CookieConsent.tsx` con 2 opzioni ("Accetta tutti" / "Solo necessari") salvate in localStorage. **MANCANO**: audit trail persistente, consenso esplicito per data collection (nome, email, phone), consenso per interview recording/storage
- [‚úó] **Data retention**: **NON IMPLEMENTATA** ‚Äî Nessuna policy di retention. Campi `expiresAt` esistono nello schema ma nessun cron di purging automatico. Dati persistono indefinitamente
- [‚úó] **Right to deletion**: **NON IMPLEMENTATO PER END USER** ‚Äî Solo `deleteUser()` admin action (admin.ts:288). Nessun user self-service deletion, nessun data export API (GDPR Art. 20 Right of Portability), nessun "right to be forgotten"
- [~] **Cookie consent**: FUNZIONANTE ma basico ‚Äî localStorage senza scadenza, nessun consenso granulare (analytics vs functional), stato consent non validato prima di inviare analytics

### 14.3 Sicurezza LLM

- [!] **Prompt injection**: **CRITICO** ‚Äî Input utente NON sanitizzato prima di injection in prompt. Solo 6 pattern regex basici in `PROMPT_LEAK_PATTERNS` (chatbot/message/route.ts:30-37) applicati solo all'OUTPUT, non all'INPUT. Concatenazione diretta: `[User says: "${userMessage}"]` in chat/route.ts:2927. Mancano pattern per: "ignore instructions", "pretend", "forget", "simulate", unicode evasion, encoding tricks
- [~] **Data exfiltration**: Organization isolation tramite `organizationId` nelle query. MA P29 (SERP data) e P30 (visibility prompts) iniettano dati esterni non controllati. Il chatbot ha scope guardrails ("STRICT SCOPE") ma nessun check output-side per leak di system prompt
- [‚úì] **API key protection**: VERIFICATO ‚Äî Keys isolate server-side (`getApiKey()` in llmService.ts). Mai inviate al client, mai loggate. Fallback chain: Bot-specific ‚Üí GlobalConfig (cache 5min) ‚Üí env var. Nessuna esposizione in error messages
- [‚úì] **Rate limiting**: VERIFICATO ‚Äî Global: 20 req/10s via Upstash Redis (middleware.ts). Message cooldown: 250ms via in-memory Map (rateLimiter.ts). Eccezioni: /api/auth, /api/stripe/webhook. **NOTA**: in-memory cooldown non distribuito (serverless multi-instance), Redis rate limit fails open (accetta se Redis down)

---

<a id="15-performance"></a>
## 15. AUDIT CROSS-CUTTING: PERFORMANCE & COSTI

> üè∑Ô∏è **Modello:** Sonnet per analisi, Haiku per check

### 15.1 Latenza

- [‚úì] **Chat response time**: VERIFICATO ‚Äî `maxDuration=60` (chat/route.ts:43), hard timeout 45s (line 2755) con fallback response `{ type: 'timeout_fallback' }`. Chatbot: `maxDuration=30`. Copilot: 45s AbortSignal
- [‚úì] **Parallel operations**: VERIFICATO ‚Äî **3 pattern Promise.all() strategici** in chat/route.ts: (1) saveMessage+updateProgress+getApiKey+getModels in parallelo, (2) extractProfile+completeInterview, (3) saveMessage+updateState. Riduzione latenza stimata ~20-30%
- [‚úì] **Runtime knowledge timeout**: VERIFICATO ‚Äî 1400ms timeout (runtime-knowledge.ts) con fallback deterministico se LLM non risponde. Fallback produce output usabile
- [‚úì] **Candidate extraction timeout**: VERIFICATO ‚Äî 10s timeout con caching in-memory + check DB per prevenire extraction ridondanti
- [‚úì] **Global config cache**: VERIFICATO ‚Äî Cache 5 minuti in-memory per API keys e provider config (llmService.ts:34-37). Riduce query DB da ogni richiesta a ogni 5min per istanza

### 15.2 Costi LLM

- [‚úì] **Model routing efficiency**: VERIFICATO ‚Äî **Multi-tier routing eccellente**: primary (gpt-4o-mini/claude-sonnet), critical (gpt-4o per escalation), dataCollection (modello pi√π economico per lead extraction). 3.5-5x risparmio su task data collection. Configurabile via env vars per provider
- [~] **Prompt token count**: Interview system prompt (7 blocchi) stima ~800-1200 tokens base + context dinamico. P27 (CrossChannel Insight) √® il pi√π costoso (~2000 tokens). P28 (CMS) ~600-900 tokens. **NOTA**: injection condizionale dei blocchi riduce costo medio
- [~] **Unnecessary LLM calls**: P26 (Analytics Aggregation) usa gpt-4o dove gpt-4o-mini basterebbe. P10 (Micro-Planner) √® gi√† deterministico (buona pratica). Smart Lead Decision (P22) potrebbe essere pi√π deterministico
- [~] **Cross-channel sync cost**: Prompt ~2000 tokens + dati multicanale. Sostenibile come batch job (non real-time), MA se eseguito frequentemente su molte org potrebbe diventare costoso
- [~] **Batch vs real-time**: Analytics, gaps, CMS suggestions sono correttamente progettati come batch (cron). MA **nessun cron √® schedulato** (no vercel.json), quindi tutto funziona solo su trigger manuale o esterno

### 15.3 Scalabilit√†

- [~] **Serverless limits**: chat/route.ts √® **3511 righe** ‚Äî file molto grande che impatta cold start (parsing/compilation). Funziona con maxDuration=60 ma **raccomandato split in moduli** per ridurre cold start e manutenibilit√†
- [~] **DB connection pooling**: Prisma usa `@prisma/adapter-pg` con Node.js `pg` Pool. Pool size **default (10)** senza config esplicita. Singleton pattern previene istanze multiple. **Potenzialmente insufficiente** per produzione con richieste concorrenti
- [!] **In-memory state**: **CRITICO** ‚Äî Due anti-pattern serverless: (1) `notificationsSent = new Map()` in creditNotificationService ‚Äî reset su deploy, causa email duplicate. (2) `cooldownStore = new Map()` in rateLimiter ‚Äî non distribuito, bypass possibile su istanze multiple. **FIX: migrare a Redis**
- [~] **Cron job reliability**: I cron jobs hanno try/catch e logging, MA nessun retry mechanism, nessun dead letter queue, nessun monitoring. **E soprattutto: nessuno √® schedulato** (no vercel.json)

---

<a id="16-gap"></a>
## 16. GAP ANALYSIS & ROADMAP SUGGERITA

> üè∑Ô∏è **Modello:** Opus ‚Äî sintesi di tutti i findings confermati (Sez. 2-15)

### SOMMARIO AUDIT

| Metrica | Valore |
|---------|--------|
| Verifiche totali eseguite | ~200 |
| Items ‚úì (confermati funzionanti) | ~128 |
| Items ~ (parziali/con caveat) | ~42 |
| Items ‚úó (non funzionanti/assenti) | ~18 |
| Items ! (critici/bloccanti) | ~12 |
| Prompt auditati | 30 (media qualit√†: 6.9/10) |
| Cron jobs trovati | 10 (0 schedulati) |
| Vulnerabilit√† sicurezza | 6 (2 CRITICHE, 2 ALTE, 2 MEDIE) |

---

### 16.1 Gap Critici (Severit√†: üî¥ BLOCCANTE)

#### A. Cron Jobs Non Schedulati ‚Äî TUTTI E 10 INATTIVI
- [!] **Gap confermato (Sez. 2.1, 4.1, 5.1, 6.1, 7.1, 10.1)**: **Nessun `vercel.json` esiste nel progetto**. I 10 cron jobs (fee-partner, kb-growth, analytics-aggregate, serp-monitoring, tip-generation, insight-sync, cms-suggestions, credits-reset, chatbot-analytics, attribution-check) hanno endpoint funzionanti ma **nessuno √® mai eseguito**
- **Impatto**: CATASTROFICO ‚Äî KB non cresce, analytics non si aggregano, fee partner non calcolate, crediti non resettati, suggerimenti CMS mai generati, monitoring SERP fermo
- **Fix**: Creare `vercel.json` con schedule per tutti i 10 cron; unificare auth (attualmente 3 metodi diversi: Bearer, x-cron-secret, condizionale)
- **Effort stimato**: 2-4 ore

#### B. In-Memory State su Serverless ‚Äî ANTI-PATTERN CRITICO
- [!] **Gap confermato (Sez. 14.1, 15.3)**: Due `Map()` in-memory che si perdono ad ogni cold start/deploy:
  - `notificationsSent` in `creditNotificationService.ts` ‚Üí email duplicate o mancanti
  - `cooldownStore` in `rateLimiter.ts` ‚Üí rate limit bypassabile su istanze multiple
- **Impatto**: CRITICO ‚Äî Email duplicate ai clienti, rate limiting inefficace, comportamento non deterministico
- **Fix**: Migrare entrambi a Redis (Upstash gi√† usato per middleware rate limiter)
- **Effort stimato**: 3-4 ore

#### C. PII nei Log + Assenza GDPR User Deletion
- [!] **Gap confermato (Sez. 14.2)**: `console.log/error` contengono email, nomi utente, ID sessione in chiaro. **Nessun endpoint di cancellazione account utente** (GDPR Art.17 non implementato). Cookie consent solo localStorage (non tracciabile server-side). Nessuna data retention policy
- **Impatto**: CRITICO ‚Äî Violazione GDPR potenziale, rischio sanzioni, dati personali in log Vercel accessibili
- **Fix**: (1) Sanitizzare tutti i log, (2) Implementare `/api/user/delete-account` con cascade delete, (3) Migrare consenso cookie a DB, (4) Definire retention policy
- **Effort stimato**: 8-12 ore

#### D. Prompt Injection Vulnerabilities
- [!] **Gap confermato (Sez. 12, 14.3)**: Input utente concatenato direttamente nei prompt senza sanitizzazione:
  - **P29 (SERP Monitoring)**: Contenuto web da SerpAPI iniettato nel prompt senza escape ‚Äî rischio jailbreak via contenuto malevolo indicizzato
  - **P30 (Visibility Prompts)**: Input utente (brand, keywords) passati a temp 0.8 senza validazione
  - **P21-P22 (Chatbot)**: Messaggi utente non filtrati
  - Solo 6 pattern base di output sanitization (iniezione "non rivelare", "non uscire dal tema")
- **Impatto**: ALTO ‚Äî Possibile estrazione istruzioni sistema, generazione contenuti non autorizzati, prompt leaking
- **Fix**: Implementare input sanitization layer pre-prompt; separare user content con delimitatori; aggiungere output validation post-LLM
- **Effort stimato**: 6-8 ore

#### E. Bug P5: Language Detection Sempre True
- [!] **Gap confermato (Sez. 12)**: In `prompt-builder.ts` il check `isItalian = label.length > 0` √® **sempre true** (label non √® mai vuoto). Tutte le interviste ricevono il prompt italiano indipendentemente dalla lingua configurata
- **Impatto**: ALTO ‚Äî Le interviste in lingue diverse dall'italiano ricevono istruzioni miste/errate
- **Fix**: Correggere condizione a `isItalian = language === 'it'` o equivalente basato su campo lingua effettivo
- **Effort stimato**: 30 minuti

---

### 16.2 Gap Funzionali Alti (Severit√†: üü† SIGNIFICATIVO)

#### F. Knowledge Base: Solo 2 di 7 Fonti Auto-Growth Attive
- [~] **Gap confermato (Sez. 5)**: Le 7 fonti dichiarate per auto-growth KB sono: (1) Conversazioni interviste, (2) Conversazioni chatbot, (3) Analytics aggregati, (4) Tip implementati, (5) Contenuti CMS pubblicati, (6) Dati SERP, (7) Feedback utente. **Solo conversazioni (1) e analytics (3) hanno pipeline funzionanti**. Le altre 5 fonti non alimentano la KB
- **Impatto**: La piattaforma non migliora autonomamente nel tempo. La KB rimane statica
- **Fix**: Implementare pipeline di estrazione per le 5 fonti mancanti; prerequisito: attivare i cron jobs (Gap A)
- **Effort stimato**: 15-20 ore

#### G. KB Senza Ricerca Semantica (Text-Only RAG)
- [~] **Gap confermato (Sez. 5.2)**: La KB usa solo ricerca keyword-based su testo raw. Nessun vector embedding, nessun similarity search semantico. `searchPlatformKB()` fa text matching letterale
- **Impatto**: Rilevanza risposte KB limitata, miss su query semanticamente simili ma lessicalmente diverse
- **Fix**: Implementare pgvector (gi√† su PostgreSQL) o servizio esterno (Pinecone, Weaviate). Generare embeddings per ogni entry KB
- **Effort stimato**: 12-16 ore

#### H. Internazionalizzazione Assente
- [~] **Gap confermato (Sez. 13)**: Nessun framework i18n (next-intl, react-i18next). Stringhe UI hardcoded in italiano. 5 prompt ausiliari Italian-only (P11 tone adapter, P25 gap detector, P27 sync engine, P28 CMS generator, P30 visibility). Email templates Italian-only. Copilot Italian-only
- **Impatto**: Blocca espansione internazionale. Utenti non-italiani hanno UX degradata
- **Fix**: (1) Adottare next-intl, (2) Estrarre tutte le stringhe in file di traduzione, (3) Aggiungere parametro `language` ai 5 prompt ausiliari, (4) Template email multilingue
- **Effort stimato**: 30-40 ore (refactor significativo)

#### I. Prompt Quality ‚Äî Extraction/Analytics Score 4.7/10
- [~] **Gap confermato (Sez. 12)**: I 30 prompt hanno media 6.9/10, ma la categoria Extraction/Analytics √® criticamente bassa (4.7/10):
  - P20 (Candidate Extractor): **Manca temperature** ‚Äî usa default 1.0, output inconsistente per task deterministico
  - P25 (Knowledge Gap Detector): Prompt minimale, 4/10
  - P26 (Chatbot Analytics Aggregator): Usa gpt-4o (troppo costoso per task semplice), 5/10
  - P11 (Tone Adapter): Italian-only, mapping rigido, 5/10
- **Fix**: (1) Aggiungere `temperature: 0.2` a P20, (2) Arricchire P25 con criteri specifici, (3) Downgrade P26 a gpt-4o-mini, (4) Rendere P11 multilingue
- **Effort stimato**: 4-6 ore

#### J. AI Tips Non Implementabili End-to-End
- [~] **Gap confermato (Sez. 8)**: I tip sono generati e visualizzabili ma il flusso edit ‚Üí preview ‚Üí approve ‚Üí publish √® incompleto. CMS pipeline funziona per WordPress ma social (n8n) non completamente implementato. Nessun editor inline per modificare tip prima della pubblicazione
- **Impatto**: L'utente deve copiare manualmente contenuti suggeriti e pubblicarli
- **Fix**: Implementare editor inline con preview, completare n8n social workflows, aggiungere status tracking per tip implementati
- **Effort stimato**: 15-20 ore

---

### 16.3 Gap di Allineamento Strategico (Severit√†: üü° MIGLIORATIVO)

#### K. Social Media Integration Incompleta
- [~] **Gap confermato (Sez. 7, 8)**: WordPress MCP publishing funziona (push contenuti). WooCommerce MCP funziona per discovery prodotti. **Ma**: n8n integration per social (LinkedIn, Facebook, Instagram) √® strutturalmente presente (schema DB `N8NConnection`) ma workflow non completamente implementati. Nessun template post pre-formattato per social
- **Fix**: Completare workflow n8n; creare template per LinkedIn (carousel, article), Facebook, Instagram; implementare approval flow
- **Effort stimato**: 12-16 ore

#### L. GEO Monitoring ‚Äî Copertura Provider Limitata
- [~] **Gap confermato (Sez. 6)**: Visibility Engine interroga 3 provider (OpenAI, Anthropic, Gemini) in parallelo. **Mancano**: Perplexity, Bing Chat/Copilot, You.com. SerpAPI AI Overview detection funziona ma con heuristic (non API ufficiale)
- **Fix**: Aggiungere Perplexity API e Bing Chat; implementare tracking temporale citazioni per trend analysis
- **Effort stimato**: 8-10 ore

#### M. LinkedIn B2B Strategy Non Ottimizzata
- [~] **Gap confermato (Sez. 7, 8)**: I suggerimenti CMS generano 5 tipi contenuto (blog_post, landing_page, faq, product_description, social_post) ma senza ottimizzazione specifica per LinkedIn B2B (formati carousel, document, newsletter, poll). Italian PMI target non ha prompt enrichment specifico
- **Fix**: Aggiungere formati LinkedIn-specifici; enrichire prompt CMS con contesto mercato italiano, incentivi digitali, normativa
- **Effort stimato**: 4-6 ore

#### N. Reporting & Export Assenti
- [~] **Gap confermato (Sez. 11)**: Dashboard mostra analytics in-app ma **nessun export** (PDF, PowerPoint, CSV). Nessun report periodico automatico. Per PMI italiane la reportistica per management √® fondamentale
- **Fix**: Implementare export PDF con metriche chiave; report periodico via email; template PowerPoint con insights
- **Effort stimato**: 10-14 ore

---

### 16.4 Gap Tecnici/Infrastrutturali (Severit√†: üîµ DEBITO TECNICO)

#### O. File chat/route.ts ‚Äî 3511 Righe
- [~] **Gap confermato (Sez. 15.3)**: Il file principale delle interviste √® **3511 righe** ‚Äî impatta cold start Vercel, manutenibilit√†, code review. Contiene supervisor, context manager, enforcement, validation, streaming tutto in un file
- **Fix**: Estrarre in moduli: `supervisor.ts`, `context-manager.ts`, `enforcement.ts`, `response-builder.ts`
- **Effort stimato**: 8-10 ore (refactoring)

#### P. Database Pool Size Default
- [~] **Gap confermato (Sez. 15.3)**: Prisma usa `@prisma/adapter-pg` con pool size **default (10)**. Per produzione con utenti concorrenti potrebbe essere insufficiente. Nessun monitoring connessioni attive
- **Fix**: Configurare pool size esplicito (20-30); aggiungere health check endpoint; monitoring connessioni
- **Effort stimato**: 2-3 ore

#### Q. Auth Cron Inconsistente ‚Äî 3 Metodi Diversi
- [~] **Gap confermato (Sez. 2.1, 10.1)**: I 10 cron usano 3 metodi auth diversi: (1) `Bearer CRON_SECRET` header, (2) `x-cron-secret` header, (3) condizionale su env. Nessuno standard
- **Fix**: Unificare a un solo metodo (raccomandato: `Authorization: Bearer ${CRON_SECRET}`)
- **Effort stimato**: 1-2 ore

#### R. Feedback Loop Utente Assente
- [~] **Gap confermato (Sez. 3, 8)**: Nessun meccanismo thumbs up/down su: risposte chatbot, suggerimenti AI, contenuti CMS generati. Nessun ciclo di miglioramento basato su feedback esplicito
- **Fix**: Aggiungere rating component su chatbot responses e tip cards; pipeline per rinforzare prompt basati su feedback
- **Effort stimato**: 6-8 ore

---

### 16.5 ROADMAP PRIORITIZZATA

Ordine di implementazione suggerito per massimizzare impatto con minimo effort:

| Priorit√† | Gap | Effort | Impatto | Sprint |
|----------|-----|--------|---------|--------|
| üî¥ P0 | **E. Fix bug P5 lingua** | 30 min | Corregge tutte le interviste non-italiane | Sprint 1 |
| üî¥ P0 | **A. Creare vercel.json** | 2-4h | Attiva tutti i 10 cron, sblocca KB growth, analytics, billing | Sprint 1 |
| üî¥ P0 | **Q. Unificare auth cron** | 1-2h | Prerequisito per A, sicurezza | Sprint 1 |
| üî¥ P1 | **B. Migrare Map() a Redis** | 3-4h | Elimina email duplicate, rende rate limit affidabile | Sprint 1 |
| üî¥ P1 | **D. Input sanitization LLM** | 6-8h | Previene prompt injection su 4+ endpoint | Sprint 1 |
| üî¥ P1 | **C. GDPR compliance** | 8-12h | User deletion, log sanitization, consent tracking | Sprint 2 |
| üü† P2 | **I. Fix prompt quality** | 4-6h | P20 temperatura, P25-P26 miglioramento, P11 multilingue | Sprint 2 |
| üü† P2 | **F. KB auto-growth 5 fonti** | 15-20h | La piattaforma migliora autonomamente | Sprint 2-3 |
| üü† P2 | **J. AI Tips end-to-end** | 15-20h | Utente pu√≤ implementare suggerimenti direttamente | Sprint 3 |
| üü† P3 | **G. KB semantic search** | 12-16h | Rilevanza risposte KB migliora significativamente | Sprint 3 |
| üü° P3 | **K. Social integration** | 12-16h | Automazione social media completa | Sprint 3-4 |
| üü° P3 | **O. Split chat/route.ts** | 8-10h | Manutenibilit√†, cold start ridotti | Sprint 4 |
| üü° P4 | **L. GEO providers** | 8-10h | Copertura monitoring pi√π completa | Sprint 4 |
| üü° P4 | **M. LinkedIn B2B** | 4-6h | Contenuti ottimizzati per target PMI | Sprint 4 |
| üü° P4 | **N. Reporting/Export** | 10-14h | Valore per management PMI | Sprint 4-5 |
| üîµ P4 | **H. Internazionalizzazione** | 30-40h | Espansione mercati non-italiani | Sprint 5-6 |
| üîµ P5 | **P. DB pool tuning** | 2-3h | Resilienza sotto carico | Sprint 5 |
| üîµ P5 | **R. Feedback loop** | 6-8h | Miglioramento continuo basato su utente | Sprint 5 |

**Sprint 1 (Week 1-2)**: Fix critici immediati ‚Äî ~15-20h ‚Üí Piattaforma stabile e sicura
**Sprint 2 (Week 3-4)**: GDPR + Prompt quality + Inizio KB growth ‚Äî ~27-38h ‚Üí Compliance e qualit√†
**Sprint 3 (Week 5-7)**: KB completa + Tips end-to-end ‚Äî ~27-36h ‚Üí Valore automatico
**Sprint 4 (Week 8-10)**: Social + GEO + Refactoring ‚Äî ~30-42h ‚Üí Feature complete
**Sprint 5-6 (Week 11-16)**: i18n + Ottimizzazioni ‚Äî ~38-51h ‚Üí Espansione internazionale

---

## PIANO DI ESECUZIONE

### Fase 1: Check Strutturali (Haiku) - ~2h
- Verifica esistenza file, pattern matching DB ‚Üî API ‚Üî UI
- Verifica schema Prisma vs campi effettivamente usati
- Inventario completo route API vs pagine UI

### Fase 2: Verifica UI ‚Üî Codice (Sonnet) - ~4h
- Per ogni area: mappatura campi UI ‚Üí logica backend
- Test di corrispondenza label/azioni/risultati
- Verifica stati vuoti, errori, edge case

### Fase 3: Audit Prompt & LLM (Opus) - ~6h
- Analisi qualitativa di ogni prompt (30 prompt totali)
- Verifica guardrail e temperature
- Test di robustezza (injection, hallucination, leaking)
- Confronto con best practices

### Fase 4: Audit Pipeline Dati (Sonnet) - ~3h
- Tracciamento flusso dati end-to-end per ogni feature
- Verifica cron jobs e aggregazioni
- Verifica crediti e billing

### Fase 5: Ricerca & Confronto Best Practices (Opus) - ~4h
- Web research su tutte le 6 aree tematiche
- Confronto piattaforma vs benchmark industry
- Identificazione gap rispetto a stato dell'arte

### Fase 6: Gap Analysis & Raccomandazioni (Opus) - ~3h
- Sintesi tutti i gap trovati
- Prioritizzazione per impatto business
- Roadmap implementativa suggerita con effort stimato

---

## KPI DI AUDIT

L'audit monitorer√† l'allineamento con i KPI piattaforma dichiarati:

| KPI | Come verificare |
|-----|----------------|
| Tempo su piattaforma | Verificare che UX sia fluida e non frustrante |
| Token utilizzati | Verificare che l'ottimizzazione modelli riduca costi |
| Tool creati | Verificare che la creazione bot/interviste sia semplice |
| Feedback positivi | Verificare qualit√† interviste e accuratezza insight |

---

> **Note finali:**
> Questo piano copre ~200 verifiche individuali organizzate in 16 aree.
> Tempo stimato totale: ~22 ore di audit con mix di modelli.
> Priorit√† assoluta: Sezioni 2 (Interview), 5 (KB), 8 (Tips), 12 (Prompts).
